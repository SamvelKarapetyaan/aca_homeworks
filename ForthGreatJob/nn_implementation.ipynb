{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X_diab, y_diab = load_diabetes(return_X_y=True) # returns diabetes data shapes: (442, 10) and (442,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a neural network layer.\n",
    "    ==========\n",
    "    Attributes:\n",
    "    ----------\n",
    "        units (int): Number of neurons in the layer.\n",
    "        input_layer (bool): Whether the layer is an input layer.\n",
    "        activation (str): Activation function for the layer.\n",
    "        use_bias (bool): Whether to use bias in the layer.\n",
    "        optimizer (str): Optimization algorithm used for the layer.\n",
    "        w (numpy.ndarray): Weights matrix for the layer.\n",
    "\n",
    "    Methods:\n",
    "    ---------\n",
    "        activationFunction(z):\n",
    "            Apply the activation function to the given input.\n",
    "\n",
    "        call(X):\n",
    "            Perform a forward pass through the layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            units, \n",
    "            *, \n",
    "            input_layer: bool = False,\n",
    "            activation: str = \"linear\",\n",
    "            use_bias: bool = True,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer.\n",
    "\n",
    "        Args:\n",
    "            units (int): Count of neurons in the layer.\n",
    "            input_layer (bool, optional): Whether the layer is an input layer. Defaults to False.\n",
    "            activation (str, optional): Activation function for the layer. Can be \"linear\", \"relu\", or \"sigmoid\". Defaults to \"linear\".\n",
    "            use_bias (bool, optional): Whether to use bias in the layer. Defaults to True.\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        self.units = units\n",
    "        self.input_layer = input_layer\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.optimizer = None # Optimizer for layer\n",
    "\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "\n",
    "        self.w = None # Weights matrix\n",
    "        self._weight_gradient = None # Weights derivative matrix\n",
    "        self._bias_gradient = None # Biases derivative vector\n",
    "\n",
    "    def activationFunction(self, z):\n",
    "        \"\"\"\n",
    "        Apply the activation function to the given input.\n",
    "\n",
    "        Args:\n",
    "            z (numpy.ndarray): Input to the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output after applying the activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return z\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(z, np.zeros(z.shape))\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _weightInit(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the weights matrix based on the input size.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of the input.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return # input_layer doesn't need weights\n",
    "\n",
    "        self.w = np.random.normal(loc = 0, scale = 1 / input_size, size=(input_size, self.units))\n",
    "        # Initialize weights matrix using a normal distribution with mean 0 and variance 1 / input_size\n",
    "\n",
    "        self.bias = np.zeros((1, self.units))\n",
    "        # Initialize biases as zeros\n",
    "\n",
    "\n",
    "    def _setOptimizer(self, optimizer, beta_1, beta_2):\n",
    "        \"\"\"\n",
    "        Set the optimizer and initialize optimizer-specific variables.\n",
    "\n",
    "        Args:\n",
    "            optimizer (str): Optimization algorithm to use.\n",
    "            beta_1 (float): Value for the optimizer parameter beta_1.\n",
    "            beta_2 (float): Value for the optimizer parameter beta_2.\n",
    "\n",
    "        Notes:\n",
    "            - Only executed for layers other than the input layer.\n",
    "            - Sets the optimizer and initializes optimizer-specific variables based on the chosen optimizer.\n",
    "            - For each optimizer, the corresponding variables are initialized.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self._b1 = beta_1\n",
    "        self._b2 = beta_2\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "            # Initialize weight-specific variables for AdaGrad\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "                # Initialize bias-specific variables for AdaGrad\n",
    "\n",
    "        if self.optimizer == 'adam':\n",
    "            self._iter = 0  # Calculate iterations\n",
    "\n",
    "            self._weight_m = np.zeros(self.w.shape)\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "            # Initialize weight-specific variables for Adam\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = np.zeros(self.bias.shape)\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "                # Initialize bias-specific variables for Adam\n",
    "\n",
    "        if self.optimizer == 'rms_prop':\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "            # Initialize weight and bias-specific variables for RMSprop\n",
    "\n",
    "        if self.optimizer == 'gdm':\n",
    "            self._weight_m = np.zeros(self.w.shape)\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = np.zeros(self.bias.shape)\n",
    "            # Initialize weight and bias-specific variables for Gradient Descent with Momentum   \n",
    "\n",
    "    def _activationDerivative(self):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the activation function.\n",
    "\n",
    "        Notes:\n",
    "            Only supports the \"linear\", \"relu\", and \"sigmoid\" activation functions.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return 1\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return (self._output > 0) * 1\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self._output * (1 - self._output)\n",
    "\n",
    "    def _setGrad(self, grad):\n",
    "        \"\"\"\n",
    "        Calculate the gradients of weights and bias for backpropagation.\n",
    "\n",
    "        Args:\n",
    "            grad (numpy.ndarray): Gradient from the previous layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient to be passed to the previous layer.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "        \n",
    "        grad = grad * self._activationDerivative()\n",
    "        self._weight_gradient = self._input.T @ grad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self._bias_gradient = grad.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return grad @ self.w.T\n",
    "    \n",
    "    def _updateGrad(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and bias based on the computed gradients.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "\n",
    "        Notes:\n",
    "            - Only executed for layers other than the input layer.\n",
    "            - Updates the weights and biases based on the computed gradients and the chosen optimizer.\n",
    "            - For each optimizer, the corresponding update rule is applied.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        eps = 10e-8 # Optimizer's epsilon\n",
    "\n",
    "        if self.optimizer == \"gd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._weight_v += np.square(self._weight_gradient)\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._weight_v) + eps)\n",
    "\n",
    "            self.w -= learning_rate_weight * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v += np.square(self._bias_gradient)\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._bias_v) + eps)\n",
    "\n",
    "                self.bias -= learning_rate_bias * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == 'adam':\n",
    "            self._iter += 1\n",
    "\n",
    "            self._weight_m = self._b1 * self._weight_m + (1- self._b1) * self._weight_gradient\n",
    "            self._weight_v = self._b2 * self._weight_v + (1- self._b2) * np.square(self._weight_gradient)\n",
    "\n",
    "            weight_m = self._weight_m / (1 - np.power(self._b1, self._iter))\n",
    "            weight_v = self._weight_v / (1 - np.power(self._b2, self._iter))\n",
    "\n",
    "            self.w -= learning_rate * weight_m / (np.sqrt(weight_v) + eps) # Updating\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = self._b1 * self._bias_m + (1- self._b1) * self._bias_gradient\n",
    "                self._bias_v = self._b2 * self._bias_v + (1- self._b2) * np.square(self._bias_gradient)\n",
    "\n",
    "                bias_m = self._bias_m / (1 - np.power(self._b1, self._iter)) \n",
    "                bias_v = self._bias_v / (1 - np.power(self._b2, self._iter))\n",
    "\n",
    "\n",
    "                self.bias -= learning_rate * bias_m / (np.sqrt(bias_v) + eps) # Updating\n",
    "\n",
    "        \n",
    "        if self.optimizer == 'rms_prop':\n",
    "            self._weight_v = self._b2 * self._weight_v + (1- self._b2) * np.square(self._weight_gradient)\n",
    "\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._weight_v) + eps)\n",
    "\n",
    "            self.w -= learning_rate_weight * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = self._b2 * self._bias_v + (1- self._b2) * np.square(self._bias_gradient)\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._bias_v) + eps)\n",
    "\n",
    "                self.bias -= learning_rate_bias * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == 'gdm':\n",
    "            self._weight_m = self._b2 * self._weight_m + (1 - self._b2) * self._weight_gradient\n",
    "\n",
    "            self.w -= learning_rate * self._weight_m\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = self._b2 * self._bias_m + (1 - self._b2) * self._bias_gradient\n",
    "\n",
    "                self.bias -= learning_rate * self._bias_m\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input to the layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output of the layer after applying the activation function.\n",
    "        \"\"\"\n",
    "        if self.input_layer:\n",
    "            return X\n",
    "        \n",
    "        self._input = X\n",
    "        self._output = self.activationFunction(X @ self.w + self.bias)\n",
    "\n",
    "        return self._output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeauralNetwork:\n",
    "    \"\"\"\n",
    "    Neural Network\n",
    "    ==============\n",
    "\n",
    "    A neural network model for deep learning.\n",
    "\n",
    "    The `NeuralNetwork` class allows you to create and train a neural network model with customizable architecture and\n",
    "    training parameters.\n",
    "\n",
    "    Args:\n",
    "    -------\n",
    "        layers (list): List of Layer objects defining the network architecture.\n",
    "        loss_function (str, optional): Loss function to use. Defaults to \"mse\".\n",
    "        learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n",
    "        verbose (bool, optional): Whether to display training progress. Defaults to False.\n",
    "        optimizer (str, optional): Optimization algorithm to use for updating weights during training. Defaults to \"gd\".\n",
    "        epochs (int, optional): Number of epochs for training. Defaults to 1.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "        beta_1 (float, optional): Parameter for the optimizer. Defaults to 0.9.\n",
    "        beta_2 (float, optional): Parameter for the optimizer. Defaults to 0.999.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "        __init__(self, layers, loss_function=\"mse\", learning_rate=0.01, verbose=False, optimizer=\"gd\", epochs=1,\n",
    "                 batch_size=32, beta_1=0.9, beta_2=0.999)\n",
    "            Initializes a neural network object.\n",
    "        lossFunction(self, y_true, y_pred)\n",
    "            Compute the loss between the true values and predicted values.\n",
    "        fit(self, X, y)\n",
    "            Train the neural network on the given input-output pairs.\n",
    "        predict(self, X)\n",
    "            Perform predictions using the trained neural network.\n",
    "        forward(self, X)\n",
    "            Perform a forward pass through the network.\n",
    "        backward(self, y_pred, y_true)\n",
    "            Perform backpropagation to update the weights of the network.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            layers: list, \n",
    "            loss_function: str = \"mse\", \n",
    "            learning_rate = 0.01,\n",
    "            verbose: bool = False,\n",
    "            optimizer: str = \"gd\",\n",
    "            epochs: int = 1, \n",
    "            batch_size: int = 32,\n",
    "            beta_1: float = 0.9,\n",
    "            beta_2: float = 0.999\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network.\n",
    "        --------\n",
    "        Args:\n",
    "        --------\n",
    "            layers (list): List of Layer objects defining the network architecture. \n",
    "            loss_function (str, optional): Loss function to use. Defaults to \"mse\".\n",
    "            optimizer (str, optional): Optimization algorithm to use for updating weights during training.\n",
    "                Options include:\n",
    "                - \"gd\" (Gradient Descent): Standard gradient descent.\n",
    "                - \"sgd\" (Stochastic Gradient Descent): Update weights using a single sample at a time.\n",
    "                - \"adagrad\" (Adaptive Gradient): Adjust the learning rate based on the frequency of feature occurrences.\n",
    "                - \"adam\" (Adam): Adaptive Moment Estimation algorithm.\n",
    "                - \"rms_prop\" (Root Mean Square Propagation): Adapt the learning rate based on the moving average of squared gradients.\n",
    "                - \"gdm\" (Gradient Descent with Momentum): Add momentum to the gradient descent algorithm.\n",
    "                Defaults to \"gd\".\n",
    "\n",
    "            learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n",
    "            epochs (int, optional): Number of epochs for training. Defaults to 1.\n",
    "            batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "            verbose (bool, optional): Whether to display training progress. Defaults to False.\n",
    "\n",
    "            beta_1 (float, optional): Parameter for the optimizer. Defaults to 0.9.\n",
    "            beta_2 (float, optional): Parameter for the optimizer. Defaults to 0.999.\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.optimizer = optimizer  # Optimizer for all layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.beta_1 = beta_1  # Optimizer parameters\n",
    "        self.beta_2 = beta_2  # Optimizer parameters\n",
    "\n",
    "        # Weights initializing:\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i]._weightInit(self.layers[i - 1].units)\n",
    "            self.layers[i]._setOptimizer(self.optimizer, self.beta_1, self.beta_2)\n",
    "            # Initialize weights for each layer and set the optimizer\n",
    "\n",
    "        # Only for SGD\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.batch_size = 1  # SGD is the same as mini-batch gradient descent when batch_size = 1\n",
    "\n",
    "\n",
    "\n",
    "    def lossFunction(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the loss between the true values and predicted values.\n",
    "        \n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True values.\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 0.5 * np.mean(np.linalg.norm(y_pred - y_true, axis=1)**2)\n",
    "\n",
    "        # Can be added\n",
    "\n",
    "    def _lossFunctionDerivative(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 1 / len(y_pred) * (y_pred - y_true)\n",
    "\n",
    "        # Can be added\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the neural network on the given input-output pairs.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "            y (numpy.ndarray): Output data.\n",
    "\n",
    "        Notes:\n",
    "            - Reshape y to a column vector (shape: (n_samples, output_size)).\n",
    "        \"\"\"\n",
    "        batch_separation = [(i, i + self.batch_size) for i in range(0, len(X), self.batch_size)] # Get batch indices\n",
    "        epoch_len = len(batch_separation)\n",
    "\n",
    "        indeces = np.arange(len(X))\n",
    "\n",
    "        for _ in range(self.epochs):    \n",
    "            np.random.shuffle(indeces) # Shuffle the training data\n",
    "\n",
    "            for iter, (i, j) in enumerate(batch_separation):\n",
    "                X_ = X[indeces[i:j]] # Get current batch\n",
    "                y_ = y[indeces[i:j]] # Get current batch\n",
    "\n",
    "                pred = self.forward(X_)\n",
    "\n",
    "                if self.verbose:\n",
    "                    process_percent = int(iter / epoch_len * 10)\n",
    "                    print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter}/{epoch_len}: [{process_percent * '=' + '>' + (10 - process_percent) * '-'}] - loss: {self.lossFunction(y_, pred)}\",end='')\n",
    "                \n",
    "                self.backward(pred, y_)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter + 1}/{epoch_len}: [{11 * '='}] - loss: {self.lossFunction(y_, pred)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform predictions using the trained neural network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted output data.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        X_ = np.copy(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            X_ = layer.call(X_)\n",
    "        return X_\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to update the weights of the network.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = self._lossFunctionDerivative(y_pred, y_true)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer._setGrad(gradient)\n",
    "            layer._updateGrad(self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/1000; Batch 1/1: [===========] - loss: 14537.227141630625\n",
      " Epoch 2/1000; Batch 1/1: [===========] - loss: 14534.901000920936\n",
      " Epoch 3/1000; Batch 1/1: [===========] - loss: 14532.57538021923\n",
      " Epoch 4/1000; Batch 1/1: [===========] - loss: 14530.24939244308\n",
      " Epoch 5/1000; Batch 1/1: [===========] - loss: 14527.922488622316\n",
      " Epoch 6/1000; Batch 1/1: [===========] - loss: 14525.594205952353\n",
      " Epoch 7/1000; Batch 1/1: [===========] - loss: 14523.264280359092\n",
      " Epoch 8/1000; Batch 1/1: [===========] - loss: 14520.933509792876\n",
      " Epoch 9/1000; Batch 1/1: [===========] - loss: 14518.602372401916\n",
      " Epoch 10/1000; Batch 1/1: [===========] - loss: 14516.27077019188\n",
      " Epoch 11/1000; Batch 1/1: [===========] - loss: 14513.938570564587\n",
      " Epoch 12/1000; Batch 1/1: [===========] - loss: 14511.605565167329\n",
      " Epoch 13/1000; Batch 1/1: [===========] - loss: 14509.271698476841\n",
      " Epoch 14/1000; Batch 1/1: [===========] - loss: 14506.936931242783\n",
      " Epoch 15/1000; Batch 1/1: [===========] - loss: 14504.601468732326\n",
      " Epoch 16/1000; Batch 1/1: [===========] - loss: 14502.265633966417\n",
      " Epoch 17/1000; Batch 1/1: [===========] - loss: 14499.92988957032\n",
      " Epoch 18/1000; Batch 1/1: [===========] - loss: 14497.594211522837\n",
      " Epoch 19/1000; Batch 1/1: [===========] - loss: 14495.258598858412\n",
      " Epoch 20/1000; Batch 1/1: [===========] - loss: 14492.923038651485\n",
      " Epoch 21/1000; Batch 1/1: [===========] - loss: 14490.587510320918\n",
      " Epoch 22/1000; Batch 1/1: [===========] - loss: 14488.252018192092\n",
      " Epoch 23/1000; Batch 1/1: [===========] - loss: 14485.91650943839\n",
      " Epoch 24/1000; Batch 1/1: [===========] - loss: 14483.580963826875\n",
      " Epoch 25/1000; Batch 1/1: [===========] - loss: 14481.245325879763\n",
      " Epoch 26/1000; Batch 1/1: [===========] - loss: 14478.909561433966\n",
      " Epoch 27/1000; Batch 1/1: [===========] - loss: 14476.573648332564\n",
      " Epoch 28/1000; Batch 1/1: [===========] - loss: 14474.237543813328\n",
      " Epoch 29/1000; Batch 1/1: [===========] - loss: 14471.901213952042\n",
      " Epoch 30/1000; Batch 1/1: [===========] - loss: 14469.564588994643\n",
      " Epoch 31/1000; Batch 1/1: [===========] - loss: 14467.227662274863\n",
      " Epoch 32/1000; Batch 1/1: [===========] - loss: 14464.890403853144\n",
      " Epoch 33/1000; Batch 1/1: [===========] - loss: 14462.552825030201\n",
      " Epoch 34/1000; Batch 1/1: [===========] - loss: 14460.214884131186\n",
      " Epoch 35/1000; Batch 1/1: [===========] - loss: 14457.87654857295\n",
      " Epoch 36/1000; Batch 1/1: [===========] - loss: 14455.537767570118\n",
      " Epoch 37/1000; Batch 1/1: [===========] - loss: 14453.198510551343\n",
      " Epoch 38/1000; Batch 1/1: [===========] - loss: 14450.858753993229\n",
      " Epoch 39/1000; Batch 1/1: [===========] - loss: 14448.518453799947\n",
      " Epoch 40/1000; Batch 1/1: [===========] - loss: 14446.177599091392\n",
      " Epoch 41/1000; Batch 1/1: [===========] - loss: 14443.836169315911\n",
      " Epoch 42/1000; Batch 1/1: [===========] - loss: 14441.494127792846\n",
      " Epoch 43/1000; Batch 1/1: [===========] - loss: 14439.151430613045\n",
      " Epoch 44/1000; Batch 1/1: [===========] - loss: 14436.80803238075\n",
      " Epoch 45/1000; Batch 1/1: [===========] - loss: 14434.463885932066\n",
      " Epoch 46/1000; Batch 1/1: [===========] - loss: 14432.11895036211\n",
      " Epoch 47/1000; Batch 1/1: [===========] - loss: 14429.773183126972\n",
      " Epoch 48/1000; Batch 1/1: [===========] - loss: 14427.426536194462\n",
      " Epoch 49/1000; Batch 1/1: [===========] - loss: 14425.078961006513\n",
      " Epoch 50/1000; Batch 1/1: [===========] - loss: 14422.730408333136\n",
      " Epoch 51/1000; Batch 1/1: [===========] - loss: 14420.380828200814\n",
      " Epoch 52/1000; Batch 1/1: [===========] - loss: 14418.030165779852\n",
      " Epoch 53/1000; Batch 1/1: [===========] - loss: 14415.678367769311\n",
      " Epoch 54/1000; Batch 1/1: [===========] - loss: 14413.32537851797\n",
      " Epoch 55/1000; Batch 1/1: [===========] - loss: 14410.971140295955\n",
      " Epoch 56/1000; Batch 1/1: [===========] - loss: 14408.615595645271\n",
      " Epoch 57/1000; Batch 1/1: [===========] - loss: 14406.258685745934\n",
      " Epoch 58/1000; Batch 1/1: [===========] - loss: 14403.900347718745\n",
      " Epoch 59/1000; Batch 1/1: [===========] - loss: 14401.540519776741\n",
      " Epoch 60/1000; Batch 1/1: [===========] - loss: 14399.179137421423\n",
      " Epoch 61/1000; Batch 1/1: [===========] - loss: 14396.81613301353\n",
      " Epoch 62/1000; Batch 1/1: [===========] - loss: 14394.451439016686\n",
      " Epoch 63/1000; Batch 1/1: [===========] - loss: 14392.084983589342\n",
      " Epoch 64/1000; Batch 1/1: [===========] - loss: 14389.716696424048\n",
      " Epoch 65/1000; Batch 1/1: [===========] - loss: 14387.346503073577\n",
      " Epoch 66/1000; Batch 1/1: [===========] - loss: 14384.974327737467\n",
      " Epoch 67/1000; Batch 1/1: [===========] - loss: 14382.600090755366\n",
      " Epoch 68/1000; Batch 1/1: [===========] - loss: 14380.22370958842\n",
      " Epoch 69/1000; Batch 1/1: [===========] - loss: 14377.845103983296\n",
      " Epoch 70/1000; Batch 1/1: [===========] - loss: 14375.464184374128\n",
      " Epoch 71/1000; Batch 1/1: [===========] - loss: 14373.080864321651\n",
      " Epoch 72/1000; Batch 1/1: [===========] - loss: 14370.69505392792\n",
      " Epoch 73/1000; Batch 1/1: [===========] - loss: 14368.306661238012\n",
      " Epoch 74/1000; Batch 1/1: [===========] - loss: 14365.91558766923\n",
      " Epoch 75/1000; Batch 1/1: [===========] - loss: 14363.521733446903\n",
      " Epoch 76/1000; Batch 1/1: [===========] - loss: 14361.124999920985\n",
      " Epoch 77/1000; Batch 1/1: [===========] - loss: 14358.725283760328\n",
      " Epoch 78/1000; Batch 1/1: [===========] - loss: 14356.322471484174\n",
      " Epoch 79/1000; Batch 1/1: [===========] - loss: 14353.916459055155\n",
      " Epoch 80/1000; Batch 1/1: [===========] - loss: 14351.507129398695\n",
      " Epoch 81/1000; Batch 1/1: [===========] - loss: 14349.094366838992\n",
      " Epoch 82/1000; Batch 1/1: [===========] - loss: 14346.678047810137\n",
      " Epoch 83/1000; Batch 1/1: [===========] - loss: 14344.258051986499\n",
      " Epoch 84/1000; Batch 1/1: [===========] - loss: 14341.83424550742\n",
      " Epoch 85/1000; Batch 1/1: [===========] - loss: 14339.406495658233\n",
      " Epoch 86/1000; Batch 1/1: [===========] - loss: 14336.974665082476\n",
      " Epoch 87/1000; Batch 1/1: [===========] - loss: 14334.538614695626\n",
      " Epoch 88/1000; Batch 1/1: [===========] - loss: 14332.098199267377\n",
      " Epoch 89/1000; Batch 1/1: [===========] - loss: 14329.653272523974\n",
      " Epoch 90/1000; Batch 1/1: [===========] - loss: 14327.203681088808\n",
      " Epoch 91/1000; Batch 1/1: [===========] - loss: 14324.749268320405\n",
      " Epoch 92/1000; Batch 1/1: [===========] - loss: 14322.289873900665\n",
      " Epoch 93/1000; Batch 1/1: [===========] - loss: 14319.825320075972\n",
      " Epoch 94/1000; Batch 1/1: [===========] - loss: 14317.3554449218\n",
      " Epoch 95/1000; Batch 1/1: [===========] - loss: 14314.88006724409\n",
      " Epoch 96/1000; Batch 1/1: [===========] - loss: 14312.399005646814\n",
      " Epoch 97/1000; Batch 1/1: [===========] - loss: 14309.91206877261\n",
      " Epoch 98/1000; Batch 1/1: [===========] - loss: 14307.419060324963\n",
      " Epoch 99/1000; Batch 1/1: [===========] - loss: 14304.919793082225\n",
      " Epoch 100/1000; Batch 1/1: [===========] - loss: 14302.414059721055\n",
      " Epoch 101/1000; Batch 1/1: [===========] - loss: 14299.901641078946\n",
      " Epoch 102/1000; Batch 1/1: [===========] - loss: 14297.382321968824\n",
      " Epoch 103/1000; Batch 1/1: [===========] - loss: 14294.855882798738\n",
      " Epoch 104/1000; Batch 1/1: [===========] - loss: 14292.322088277857\n",
      " Epoch 105/1000; Batch 1/1: [===========] - loss: 14289.78069971808\n",
      " Epoch 106/1000; Batch 1/1: [===========] - loss: 14287.231466139525\n",
      " Epoch 107/1000; Batch 1/1: [===========] - loss: 14284.674141078907\n",
      " Epoch 108/1000; Batch 1/1: [===========] - loss: 14282.108465226258\n",
      " Epoch 109/1000; Batch 1/1: [===========] - loss: 14279.534168679065\n",
      " Epoch 110/1000; Batch 1/1: [===========] - loss: 14276.950966046681\n",
      " Epoch 111/1000; Batch 1/1: [===========] - loss: 14274.35857238961\n",
      " Epoch 112/1000; Batch 1/1: [===========] - loss: 14271.75669481738\n",
      " Epoch 113/1000; Batch 1/1: [===========] - loss: 14269.145029336905\n",
      " Epoch 114/1000; Batch 1/1: [===========] - loss: 14266.523266904267\n",
      " Epoch 115/1000; Batch 1/1: [===========] - loss: 14263.891086564003\n",
      " Epoch 116/1000; Batch 1/1: [===========] - loss: 14261.24816634282\n",
      " Epoch 117/1000; Batch 1/1: [===========] - loss: 14258.594166351251\n",
      " Epoch 118/1000; Batch 1/1: [===========] - loss: 14255.928731245698\n",
      " Epoch 119/1000; Batch 1/1: [===========] - loss: 14253.251498142947\n",
      " Epoch 120/1000; Batch 1/1: [===========] - loss: 14250.562094841463\n",
      " Epoch 121/1000; Batch 1/1: [===========] - loss: 14247.860144925713\n",
      " Epoch 122/1000; Batch 1/1: [===========] - loss: 14245.145241396358\n",
      " Epoch 123/1000; Batch 1/1: [===========] - loss: 14242.416972192545\n",
      " Epoch 124/1000; Batch 1/1: [===========] - loss: 14239.674931362626\n",
      " Epoch 125/1000; Batch 1/1: [===========] - loss: 14236.918693956428\n",
      " Epoch 126/1000; Batch 1/1: [===========] - loss: 14234.147806459037\n",
      " Epoch 127/1000; Batch 1/1: [===========] - loss: 14231.361808174199\n",
      " Epoch 128/1000; Batch 1/1: [===========] - loss: 14228.56020509385\n",
      " Epoch 129/1000; Batch 1/1: [===========] - loss: 14225.742519807069\n",
      " Epoch 130/1000; Batch 1/1: [===========] - loss: 14222.908260569218\n",
      " Epoch 131/1000; Batch 1/1: [===========] - loss: 14220.056894030662\n",
      " Epoch 132/1000; Batch 1/1: [===========] - loss: 14217.187885185227\n",
      " Epoch 133/1000; Batch 1/1: [===========] - loss: 14214.300690966526\n",
      " Epoch 134/1000; Batch 1/1: [===========] - loss: 14211.394745152042\n",
      " Epoch 135/1000; Batch 1/1: [===========] - loss: 14208.46948162972\n",
      " Epoch 136/1000; Batch 1/1: [===========] - loss: 14205.524294338804\n",
      " Epoch 137/1000; Batch 1/1: [===========] - loss: 14202.558563512\n",
      " Epoch 138/1000; Batch 1/1: [===========] - loss: 14199.571655155169\n",
      " Epoch 139/1000; Batch 1/1: [===========] - loss: 14196.562893771206\n",
      " Epoch 140/1000; Batch 1/1: [===========] - loss: 14193.531616843213\n",
      " Epoch 141/1000; Batch 1/1: [===========] - loss: 14190.477132219365\n",
      " Epoch 142/1000; Batch 1/1: [===========] - loss: 14187.398700775835\n",
      " Epoch 143/1000; Batch 1/1: [===========] - loss: 14184.295589034407\n",
      " Epoch 144/1000; Batch 1/1: [===========] - loss: 14181.167022839752\n",
      " Epoch 145/1000; Batch 1/1: [===========] - loss: 14178.012230404298\n",
      " Epoch 146/1000; Batch 1/1: [===========] - loss: 14174.830381739526\n",
      " Epoch 147/1000; Batch 1/1: [===========] - loss: 14171.620633655557\n",
      " Epoch 148/1000; Batch 1/1: [===========] - loss: 14168.3821537924\n",
      " Epoch 149/1000; Batch 1/1: [===========] - loss: 14165.114072302013\n",
      " Epoch 150/1000; Batch 1/1: [===========] - loss: 14161.81546151999\n",
      " Epoch 151/1000; Batch 1/1: [===========] - loss: 14158.485358072146\n",
      " Epoch 152/1000; Batch 1/1: [===========] - loss: 14155.122804734714\n",
      " Epoch 153/1000; Batch 1/1: [===========] - loss: 14151.726806676534\n",
      " Epoch 154/1000; Batch 1/1: [===========] - loss: 14148.296350068425\n",
      " Epoch 155/1000; Batch 1/1: [===========] - loss: 14144.830344464994\n",
      " Epoch 156/1000; Batch 1/1: [===========] - loss: 14141.327677913148\n",
      " Epoch 157/1000; Batch 1/1: [===========] - loss: 14137.787199860528\n",
      " Epoch 158/1000; Batch 1/1: [===========] - loss: 14134.207809295478\n",
      " Epoch 159/1000; Batch 1/1: [===========] - loss: 14130.588272842364\n",
      " Epoch 160/1000; Batch 1/1: [===========] - loss: 14126.927342987345\n",
      " Epoch 161/1000; Batch 1/1: [===========] - loss: 14123.223730850936\n",
      " Epoch 162/1000; Batch 1/1: [===========] - loss: 14119.476158110234\n",
      " Epoch 163/1000; Batch 1/1: [===========] - loss: 14115.683256335\n",
      " Epoch 164/1000; Batch 1/1: [===========] - loss: 14111.843605517515\n",
      " Epoch 165/1000; Batch 1/1: [===========] - loss: 14107.95581557068\n",
      " Epoch 166/1000; Batch 1/1: [===========] - loss: 14104.018400815565\n",
      " Epoch 167/1000; Batch 1/1: [===========] - loss: 14100.02983236422\n",
      " Epoch 168/1000; Batch 1/1: [===========] - loss: 14095.988536019519\n",
      " Epoch 169/1000; Batch 1/1: [===========] - loss: 14091.892894141256\n",
      " Epoch 170/1000; Batch 1/1: [===========] - loss: 14087.741208475012\n",
      " Epoch 171/1000; Batch 1/1: [===========] - loss: 14083.531764166204\n",
      " Epoch 172/1000; Batch 1/1: [===========] - loss: 14079.262745279177\n",
      " Epoch 173/1000; Batch 1/1: [===========] - loss: 14074.932281587298\n",
      " Epoch 174/1000; Batch 1/1: [===========] - loss: 14070.538534700861\n",
      " Epoch 175/1000; Batch 1/1: [===========] - loss: 14066.079527604677\n",
      " Epoch 176/1000; Batch 1/1: [===========] - loss: 14061.55321893559\n",
      " Epoch 177/1000; Batch 1/1: [===========] - loss: 14056.957549366214\n",
      " Epoch 178/1000; Batch 1/1: [===========] - loss: 14052.290369293214\n",
      " Epoch 179/1000; Batch 1/1: [===========] - loss: 14047.549453928625\n",
      " Epoch 180/1000; Batch 1/1: [===========] - loss: 14042.732458393224\n",
      " Epoch 181/1000; Batch 1/1: [===========] - loss: 14037.837100042803\n",
      " Epoch 182/1000; Batch 1/1: [===========] - loss: 14032.860903285698\n",
      " Epoch 183/1000; Batch 1/1: [===========] - loss: 14027.801367068898\n",
      " Epoch 184/1000; Batch 1/1: [===========] - loss: 14022.65590281013\n",
      " Epoch 185/1000; Batch 1/1: [===========] - loss: 14017.421873540774\n",
      " Epoch 186/1000; Batch 1/1: [===========] - loss: 14012.096465731103\n",
      " Epoch 187/1000; Batch 1/1: [===========] - loss: 14006.676735505662\n",
      " Epoch 188/1000; Batch 1/1: [===========] - loss: 14001.159804071045\n",
      " Epoch 189/1000; Batch 1/1: [===========] - loss: 13995.542595681014\n",
      " Epoch 190/1000; Batch 1/1: [===========] - loss: 13989.821975418428\n",
      " Epoch 191/1000; Batch 1/1: [===========] - loss: 13983.994624683828\n",
      " Epoch 192/1000; Batch 1/1: [===========] - loss: 13978.057165767104\n",
      " Epoch 193/1000; Batch 1/1: [===========] - loss: 13972.006144829065\n",
      " Epoch 194/1000; Batch 1/1: [===========] - loss: 13965.838020867683\n",
      " Epoch 195/1000; Batch 1/1: [===========] - loss: 13959.549108056166\n",
      " Epoch 196/1000; Batch 1/1: [===========] - loss: 13953.135441796727\n",
      " Epoch 197/1000; Batch 1/1: [===========] - loss: 13946.593009286013\n",
      " Epoch 198/1000; Batch 1/1: [===========] - loss: 13939.917781034914\n",
      " Epoch 199/1000; Batch 1/1: [===========] - loss: 13933.105438427121\n",
      " Epoch 200/1000; Batch 1/1: [===========] - loss: 13926.151675409355\n",
      " Epoch 201/1000; Batch 1/1: [===========] - loss: 13919.051754259397\n",
      " Epoch 202/1000; Batch 1/1: [===========] - loss: 13911.800968976586\n",
      " Epoch 203/1000; Batch 1/1: [===========] - loss: 13904.394393286366\n",
      " Epoch 204/1000; Batch 1/1: [===========] - loss: 13896.82692928816\n",
      " Epoch 205/1000; Batch 1/1: [===========] - loss: 13889.093408326858\n",
      " Epoch 206/1000; Batch 1/1: [===========] - loss: 13881.188328258948\n",
      " Epoch 207/1000; Batch 1/1: [===========] - loss: 13873.105933597722\n",
      " Epoch 208/1000; Batch 1/1: [===========] - loss: 13864.840256709898\n",
      " Epoch 209/1000; Batch 1/1: [===========] - loss: 13856.385149014044\n",
      " Epoch 210/1000; Batch 1/1: [===========] - loss: 13847.734164558457\n",
      " Epoch 211/1000; Batch 1/1: [===========] - loss: 13838.8807515353\n",
      " Epoch 212/1000; Batch 1/1: [===========] - loss: 13829.817921674528\n",
      " Epoch 213/1000; Batch 1/1: [===========] - loss: 13820.53847972811\n",
      " Epoch 214/1000; Batch 1/1: [===========] - loss: 13811.034994335809\n",
      " Epoch 215/1000; Batch 1/1: [===========] - loss: 13801.299592970947\n",
      " Epoch 216/1000; Batch 1/1: [===========] - loss: 13791.324065013772\n",
      " Epoch 217/1000; Batch 1/1: [===========] - loss: 13781.099923192827\n",
      " Epoch 218/1000; Batch 1/1: [===========] - loss: 13770.618288924457\n",
      " Epoch 219/1000; Batch 1/1: [===========] - loss: 13759.869936080495\n",
      " Epoch 220/1000; Batch 1/1: [===========] - loss: 13748.845103125825\n",
      " Epoch 221/1000; Batch 1/1: [===========] - loss: 13737.533626985542\n",
      " Epoch 222/1000; Batch 1/1: [===========] - loss: 13725.924849570254\n",
      " Epoch 223/1000; Batch 1/1: [===========] - loss: 13714.007512432632\n",
      " Epoch 224/1000; Batch 1/1: [===========] - loss: 13701.769688295399\n",
      " Epoch 225/1000; Batch 1/1: [===========] - loss: 13689.198881143067\n",
      " Epoch 226/1000; Batch 1/1: [===========] - loss: 13676.282107355019\n",
      " Epoch 227/1000; Batch 1/1: [===========] - loss: 13663.00580805808\n",
      " Epoch 228/1000; Batch 1/1: [===========] - loss: 13649.355559586742\n",
      " Epoch 229/1000; Batch 1/1: [===========] - loss: 13635.315887762123\n",
      " Epoch 230/1000; Batch 1/1: [===========] - loss: 13620.870738306066\n",
      " Epoch 231/1000; Batch 1/1: [===========] - loss: 13606.003059514882\n",
      " Epoch 232/1000; Batch 1/1: [===========] - loss: 13590.694744019529\n",
      " Epoch 233/1000; Batch 1/1: [===========] - loss: 13574.926440640784\n",
      " Epoch 234/1000; Batch 1/1: [===========] - loss: 13558.677680571998\n",
      " Epoch 235/1000; Batch 1/1: [===========] - loss: 13541.926651075984\n",
      " Epoch 236/1000; Batch 1/1: [===========] - loss: 13524.650349282103\n",
      " Epoch 237/1000; Batch 1/1: [===========] - loss: 13506.82399645172\n",
      " Epoch 238/1000; Batch 1/1: [===========] - loss: 13488.421144675314\n",
      " Epoch 239/1000; Batch 1/1: [===========] - loss: 13469.413514547447\n",
      " Epoch 240/1000; Batch 1/1: [===========] - loss: 13449.770759525582\n",
      " Epoch 241/1000; Batch 1/1: [===========] - loss: 13429.460115657615\n",
      " Epoch 242/1000; Batch 1/1: [===========] - loss: 13408.446488435406\n",
      " Epoch 243/1000; Batch 1/1: [===========] - loss: 13386.691984811749\n",
      " Epoch 244/1000; Batch 1/1: [===========] - loss: 13364.155878121885\n",
      " Epoch 245/1000; Batch 1/1: [===========] - loss: 13340.793926030676\n",
      " Epoch 246/1000; Batch 1/1: [===========] - loss: 13316.558296319068\n",
      " Epoch 247/1000; Batch 1/1: [===========] - loss: 13291.397156647976\n",
      " Epoch 248/1000; Batch 1/1: [===========] - loss: 13265.253911787839\n",
      " Epoch 249/1000; Batch 1/1: [===========] - loss: 13238.067165867078\n",
      " Epoch 250/1000; Batch 1/1: [===========] - loss: 13209.769581207891\n",
      " Epoch 251/1000; Batch 1/1: [===========] - loss: 13180.2884985055\n",
      " Epoch 252/1000; Batch 1/1: [===========] - loss: 13149.544204551647\n",
      " Epoch 253/1000; Batch 1/1: [===========] - loss: 13117.448730625993\n",
      " Epoch 254/1000; Batch 1/1: [===========] - loss: 13083.90594101929\n",
      " Epoch 255/1000; Batch 1/1: [===========] - loss: 13048.809803155202\n",
      " Epoch 256/1000; Batch 1/1: [===========] - loss: 13012.043416001838\n",
      " Epoch 257/1000; Batch 1/1: [===========] - loss: 12973.478749739077\n",
      " Epoch 258/1000; Batch 1/1: [===========] - loss: 12932.974060632994\n",
      " Epoch 259/1000; Batch 1/1: [===========] - loss: 12890.37250777961\n",
      " Epoch 260/1000; Batch 1/1: [===========] - loss: 12845.500204575985\n",
      " Epoch 261/1000; Batch 1/1: [===========] - loss: 12798.165126073465\n",
      " Epoch 262/1000; Batch 1/1: [===========] - loss: 12748.153006606119\n",
      " Epoch 263/1000; Batch 1/1: [===========] - loss: 12695.22633784174\n",
      " Epoch 264/1000; Batch 1/1: [===========] - loss: 12639.120094287513\n",
      " Epoch 265/1000; Batch 1/1: [===========] - loss: 12579.540164044542\n",
      " Epoch 266/1000; Batch 1/1: [===========] - loss: 12516.158950651852\n",
      " Epoch 267/1000; Batch 1/1: [===========] - loss: 12448.609892670629\n",
      " Epoch 268/1000; Batch 1/1: [===========] - loss: 12376.483883874958\n",
      " Epoch 269/1000; Batch 1/1: [===========] - loss: 12299.325184739579\n",
      " Epoch 270/1000; Batch 1/1: [===========] - loss: 12216.62542444562\n",
      " Epoch 271/1000; Batch 1/1: [===========] - loss: 12127.81668360193\n",
      " Epoch 272/1000; Batch 1/1: [===========] - loss: 12032.265876939795\n",
      " Epoch 273/1000; Batch 1/1: [===========] - loss: 11929.26853038983\n",
      " Epoch 274/1000; Batch 1/1: [===========] - loss: 11818.041472720523\n",
      " Epoch 275/1000; Batch 1/1: [===========] - loss: 11697.71690855855\n",
      " Epoch 276/1000; Batch 1/1: [===========] - loss: 11567.335585692379\n",
      " Epoch 277/1000; Batch 1/1: [===========] - loss: 11425.842231703728\n",
      " Epoch 278/1000; Batch 1/1: [===========] - loss: 11272.08255251886\n",
      " Epoch 279/1000; Batch 1/1: [===========] - loss: 11104.804303572033\n",
      " Epoch 280/1000; Batch 1/1: [===========] - loss: 10922.662049435601\n",
      " Epoch 281/1000; Batch 1/1: [===========] - loss: 10724.229617843324\n",
      " Epoch 282/1000; Batch 1/1: [===========] - loss: 10508.02067932419\n",
      " Epoch 283/1000; Batch 1/1: [===========] - loss: 10272.527395952113\n",
      " Epoch 284/1000; Batch 1/1: [===========] - loss: 10016.270038810415\n",
      " Epoch 285/1000; Batch 1/1: [===========] - loss: 9737.872235408331\n",
      " Epoch 286/1000; Batch 1/1: [===========] - loss: 9436.163869231856\n",
      " Epoch 287/1000; Batch 1/1: [===========] - loss: 9110.31550061069\n",
      " Epoch 288/1000; Batch 1/1: [===========] - loss: 8760.00479683668\n",
      " Epoch 289/1000; Batch 1/1: [===========] - loss: 8385.619276894964\n",
      " Epoch 290/1000; Batch 1/1: [===========] - loss: 7988.48103006002\n",
      " Epoch 291/1000; Batch 1/1: [===========] - loss: 7571.082917567621\n",
      " Epoch 292/1000; Batch 1/1: [===========] - loss: 7137.297473187305\n",
      " Epoch 293/1000; Batch 1/1: [===========] - loss: 6692.516538172526\n",
      " Epoch 294/1000; Batch 1/1: [===========] - loss: 6243.660892180832\n",
      " Epoch 295/1000; Batch 1/1: [===========] - loss: 5798.999348090204\n",
      " Epoch 296/1000; Batch 1/1: [===========] - loss: 5367.728822632659\n",
      " Epoch 297/1000; Batch 1/1: [===========] - loss: 4959.308077751201\n",
      " Epoch 298/1000; Batch 1/1: [===========] - loss: 4582.595538379732\n",
      " Epoch 299/1000; Batch 1/1: [===========] - loss: 4244.923051731852\n",
      " Epoch 300/1000; Batch 1/1: [===========] - loss: 3951.2888485296476\n",
      " Epoch 301/1000; Batch 1/1: [===========] - loss: 3703.8543187189853\n",
      " Epoch 302/1000; Batch 1/1: [===========] - loss: 3501.8779965923\n",
      " Epoch 303/1000; Batch 1/1: [===========] - loss: 3342.0940311964832\n",
      " Epoch 304/1000; Batch 1/1: [===========] - loss: 3219.4250154998545\n",
      " Epoch 305/1000; Batch 1/1: [===========] - loss: 3127.8371300317604\n",
      " Epoch 306/1000; Batch 1/1: [===========] - loss: 3061.139970283199\n",
      " Epoch 307/1000; Batch 1/1: [===========] - loss: 3013.5937192997076\n",
      " Epoch 308/1000; Batch 1/1: [===========] - loss: 2980.2703299705345\n",
      " Epoch 309/1000; Batch 1/1: [===========] - loss: 2957.1900900037163\n",
      " Epoch 310/1000; Batch 1/1: [===========] - loss: 2941.295059741944\n",
      " Epoch 311/1000; Batch 1/1: [===========] - loss: 2930.3291764638034\n",
      " Epoch 312/1000; Batch 1/1: [===========] - loss: 2922.6823842018207\n",
      " Epoch 313/1000; Batch 1/1: [===========] - loss: 2917.236423947313\n",
      " Epoch 314/1000; Batch 1/1: [===========] - loss: 2913.2315002080704\n",
      " Epoch 315/1000; Batch 1/1: [===========] - loss: 2910.160530145614\n",
      " Epoch 316/1000; Batch 1/1: [===========] - loss: 2907.6899173691063\n",
      " Epoch 317/1000; Batch 1/1: [===========] - loss: 2905.602761719502\n",
      " Epoch 318/1000; Batch 1/1: [===========] - loss: 2903.759260892732\n",
      " Epoch 319/1000; Batch 1/1: [===========] - loss: 2902.0700096070036\n",
      " Epoch 320/1000; Batch 1/1: [===========] - loss: 2900.4781544165007\n",
      " Epoch 321/1000; Batch 1/1: [===========] - loss: 2898.9477067091807\n",
      " Epoch 322/1000; Batch 1/1: [===========] - loss: 2897.4559610390106\n",
      " Epoch 323/1000; Batch 1/1: [===========] - loss: 2895.988652809327\n",
      " Epoch 324/1000; Batch 1/1: [===========] - loss: 2894.5368182340912\n",
      " Epoch 325/1000; Batch 1/1: [===========] - loss: 2893.0948405338995\n",
      " Epoch 326/1000; Batch 1/1: [===========] - loss: 2891.659200629994\n",
      " Epoch 327/1000; Batch 1/1: [===========] - loss: 2890.2276694694096\n",
      " Epoch 328/1000; Batch 1/1: [===========] - loss: 2888.798875701905\n",
      " Epoch 329/1000; Batch 1/1: [===========] - loss: 2887.3719485373063\n",
      " Epoch 330/1000; Batch 1/1: [===========] - loss: 2885.9463501467108\n",
      " Epoch 331/1000; Batch 1/1: [===========] - loss: 2884.521729273619\n",
      " Epoch 332/1000; Batch 1/1: [===========] - loss: 2883.0978436817277\n",
      " Epoch 333/1000; Batch 1/1: [===========] - loss: 2881.674528619249\n",
      " Epoch 334/1000; Batch 1/1: [===========] - loss: 2880.251707817267\n",
      " Epoch 335/1000; Batch 1/1: [===========] - loss: 2878.829307948461\n",
      " Epoch 336/1000; Batch 1/1: [===========] - loss: 2877.4072652898903\n",
      " Epoch 337/1000; Batch 1/1: [===========] - loss: 2875.9855259232113\n",
      " Epoch 338/1000; Batch 1/1: [===========] - loss: 2874.5640707524026\n",
      " Epoch 339/1000; Batch 1/1: [===========] - loss: 2873.1428477740487\n",
      " Epoch 340/1000; Batch 1/1: [===========] - loss: 2871.721863932037\n",
      " Epoch 341/1000; Batch 1/1: [===========] - loss: 2870.3010799484287\n",
      " Epoch 342/1000; Batch 1/1: [===========] - loss: 2868.88048626538\n",
      " Epoch 343/1000; Batch 1/1: [===========] - loss: 2867.460051749488\n",
      " Epoch 344/1000; Batch 1/1: [===========] - loss: 2866.0397501091284\n",
      " Epoch 345/1000; Batch 1/1: [===========] - loss: 2864.6195533067244\n",
      " Epoch 346/1000; Batch 1/1: [===========] - loss: 2863.1994287167554\n",
      " Epoch 347/1000; Batch 1/1: [===========] - loss: 2861.7793630264296\n",
      " Epoch 348/1000; Batch 1/1: [===========] - loss: 2860.359337493753\n",
      " Epoch 349/1000; Batch 1/1: [===========] - loss: 2858.9393666768383\n",
      " Epoch 350/1000; Batch 1/1: [===========] - loss: 2857.5194326244186\n",
      " Epoch 351/1000; Batch 1/1: [===========] - loss: 2856.0995053676393\n",
      " Epoch 352/1000; Batch 1/1: [===========] - loss: 2854.679556155195\n",
      " Epoch 353/1000; Batch 1/1: [===========] - loss: 2853.2595692650457\n",
      " Epoch 354/1000; Batch 1/1: [===========] - loss: 2851.839496688767\n",
      " Epoch 355/1000; Batch 1/1: [===========] - loss: 2850.419304368474\n",
      " Epoch 356/1000; Batch 1/1: [===========] - loss: 2848.99898333921\n",
      " Epoch 357/1000; Batch 1/1: [===========] - loss: 2847.5785706940906\n",
      " Epoch 358/1000; Batch 1/1: [===========] - loss: 2846.158036315512\n",
      " Epoch 359/1000; Batch 1/1: [===========] - loss: 2844.737371181244\n",
      " Epoch 360/1000; Batch 1/1: [===========] - loss: 2843.316475181078\n",
      " Epoch 361/1000; Batch 1/1: [===========] - loss: 2841.8953515453995\n",
      " Epoch 362/1000; Batch 1/1: [===========] - loss: 2840.473898137273\n",
      " Epoch 363/1000; Batch 1/1: [===========] - loss: 2839.052182391446\n",
      " Epoch 364/1000; Batch 1/1: [===========] - loss: 2837.6301622977594\n",
      " Epoch 365/1000; Batch 1/1: [===========] - loss: 2836.207775766939\n",
      " Epoch 366/1000; Batch 1/1: [===========] - loss: 2834.7849296455074\n",
      " Epoch 367/1000; Batch 1/1: [===========] - loss: 2833.3616665575564\n",
      " Epoch 368/1000; Batch 1/1: [===========] - loss: 2831.9380128431862\n",
      " Epoch 369/1000; Batch 1/1: [===========] - loss: 2830.5139131089936\n",
      " Epoch 370/1000; Batch 1/1: [===========] - loss: 2829.0893817412107\n",
      " Epoch 371/1000; Batch 1/1: [===========] - loss: 2827.6643205727164\n",
      " Epoch 372/1000; Batch 1/1: [===========] - loss: 2826.2388015439724\n",
      " Epoch 373/1000; Batch 1/1: [===========] - loss: 2824.8127281231846\n",
      " Epoch 374/1000; Batch 1/1: [===========] - loss: 2823.3861482331317\n",
      " Epoch 375/1000; Batch 1/1: [===========] - loss: 2821.958996171587\n",
      " Epoch 376/1000; Batch 1/1: [===========] - loss: 2820.531307623396\n",
      " Epoch 377/1000; Batch 1/1: [===========] - loss: 2819.103117664923\n",
      " Epoch 378/1000; Batch 1/1: [===========] - loss: 2817.6742959205058\n",
      " Epoch 379/1000; Batch 1/1: [===========] - loss: 2816.244701238952\n",
      " Epoch 380/1000; Batch 1/1: [===========] - loss: 2814.8144666066473\n",
      " Epoch 381/1000; Batch 1/1: [===========] - loss: 2813.3835630311296\n",
      " Epoch 382/1000; Batch 1/1: [===========] - loss: 2811.9521060159454\n",
      " Epoch 383/1000; Batch 1/1: [===========] - loss: 2810.52006200898\n",
      " Epoch 384/1000; Batch 1/1: [===========] - loss: 2809.0874508318216\n",
      " Epoch 385/1000; Batch 1/1: [===========] - loss: 2807.6542236785713\n",
      " Epoch 386/1000; Batch 1/1: [===========] - loss: 2806.2204034531414\n",
      " Epoch 387/1000; Batch 1/1: [===========] - loss: 2804.7858788963076\n",
      " Epoch 388/1000; Batch 1/1: [===========] - loss: 2803.350627919567\n",
      " Epoch 389/1000; Batch 1/1: [===========] - loss: 2801.914583963678\n",
      " Epoch 390/1000; Batch 1/1: [===========] - loss: 2800.4779181141034\n",
      " Epoch 391/1000; Batch 1/1: [===========] - loss: 2799.0404075522174\n",
      " Epoch 392/1000; Batch 1/1: [===========] - loss: 2797.6019549824086\n",
      " Epoch 393/1000; Batch 1/1: [===========] - loss: 2796.1625874232254\n",
      " Epoch 394/1000; Batch 1/1: [===========] - loss: 2794.7221937938234\n",
      " Epoch 395/1000; Batch 1/1: [===========] - loss: 2793.2808284892267\n",
      " Epoch 396/1000; Batch 1/1: [===========] - loss: 2791.8384679836686\n",
      " Epoch 397/1000; Batch 1/1: [===========] - loss: 2790.3952260876454\n",
      " Epoch 398/1000; Batch 1/1: [===========] - loss: 2788.9510494996457\n",
      " Epoch 399/1000; Batch 1/1: [===========] - loss: 2787.5060059265243\n",
      " Epoch 400/1000; Batch 1/1: [===========] - loss: 2786.060099751663\n",
      " Epoch 401/1000; Batch 1/1: [===========] - loss: 2784.6133004205035\n",
      " Epoch 402/1000; Batch 1/1: [===========] - loss: 2783.1655923119692\n",
      " Epoch 403/1000; Batch 1/1: [===========] - loss: 2781.7169218652157\n",
      " Epoch 404/1000; Batch 1/1: [===========] - loss: 2780.2672370569835\n",
      " Epoch 405/1000; Batch 1/1: [===========] - loss: 2778.816476760302\n",
      " Epoch 406/1000; Batch 1/1: [===========] - loss: 2777.364729988772\n",
      " Epoch 407/1000; Batch 1/1: [===========] - loss: 2775.911919811384\n",
      " Epoch 408/1000; Batch 1/1: [===========] - loss: 2774.458098721746\n",
      " Epoch 409/1000; Batch 1/1: [===========] - loss: 2773.0031701431176\n",
      " Epoch 410/1000; Batch 1/1: [===========] - loss: 2771.547154664078\n",
      " Epoch 411/1000; Batch 1/1: [===========] - loss: 2770.090041271863\n",
      " Epoch 412/1000; Batch 1/1: [===========] - loss: 2768.631925602958\n",
      " Epoch 413/1000; Batch 1/1: [===========] - loss: 2767.1727981529307\n",
      " Epoch 414/1000; Batch 1/1: [===========] - loss: 2765.7124534723052\n",
      " Epoch 415/1000; Batch 1/1: [===========] - loss: 2764.250859511214\n",
      " Epoch 416/1000; Batch 1/1: [===========] - loss: 2762.78801250519\n",
      " Epoch 417/1000; Batch 1/1: [===========] - loss: 2761.3238888101228\n",
      " Epoch 418/1000; Batch 1/1: [===========] - loss: 2759.8583951843384\n",
      " Epoch 419/1000; Batch 1/1: [===========] - loss: 2758.39165444975\n",
      " Epoch 420/1000; Batch 1/1: [===========] - loss: 2756.9236625460203\n",
      " Epoch 421/1000; Batch 1/1: [===========] - loss: 2755.4544487140433\n",
      " Epoch 422/1000; Batch 1/1: [===========] - loss: 2753.983915832992\n",
      " Epoch 423/1000; Batch 1/1: [===========] - loss: 2752.5120924467874\n",
      " Epoch 424/1000; Batch 1/1: [===========] - loss: 2751.039142855187\n",
      " Epoch 425/1000; Batch 1/1: [===========] - loss: 2749.5650805978016\n",
      " Epoch 426/1000; Batch 1/1: [===========] - loss: 2748.089603352447\n",
      " Epoch 427/1000; Batch 1/1: [===========] - loss: 2746.612593728185\n",
      " Epoch 428/1000; Batch 1/1: [===========] - loss: 2745.134135628295\n",
      " Epoch 429/1000; Batch 1/1: [===========] - loss: 2743.6543022099495\n",
      " Epoch 430/1000; Batch 1/1: [===========] - loss: 2742.1729565859337\n",
      " Epoch 431/1000; Batch 1/1: [===========] - loss: 2740.6901676684706\n",
      " Epoch 432/1000; Batch 1/1: [===========] - loss: 2739.2058781489004\n",
      " Epoch 433/1000; Batch 1/1: [===========] - loss: 2737.7200934033267\n",
      " Epoch 434/1000; Batch 1/1: [===========] - loss: 2736.232885792658\n",
      " Epoch 435/1000; Batch 1/1: [===========] - loss: 2734.744187513755\n",
      " Epoch 436/1000; Batch 1/1: [===========] - loss: 2733.2539086170837\n",
      " Epoch 437/1000; Batch 1/1: [===========] - loss: 2731.7620663900766\n",
      " Epoch 438/1000; Batch 1/1: [===========] - loss: 2730.268634122915\n",
      " Epoch 439/1000; Batch 1/1: [===========] - loss: 2728.773544352763\n",
      " Epoch 440/1000; Batch 1/1: [===========] - loss: 2727.276893322826\n",
      " Epoch 441/1000; Batch 1/1: [===========] - loss: 2725.7785279077175\n",
      " Epoch 442/1000; Batch 1/1: [===========] - loss: 2724.2784525290035\n",
      " Epoch 443/1000; Batch 1/1: [===========] - loss: 2722.776642766917\n",
      " Epoch 444/1000; Batch 1/1: [===========] - loss: 2721.273047481301\n",
      " Epoch 445/1000; Batch 1/1: [===========] - loss: 2719.7677184506224\n",
      " Epoch 446/1000; Batch 1/1: [===========] - loss: 2718.26067039993\n",
      " Epoch 447/1000; Batch 1/1: [===========] - loss: 2716.751884636162\n",
      " Epoch 448/1000; Batch 1/1: [===========] - loss: 2715.241284565308\n",
      " Epoch 449/1000; Batch 1/1: [===========] - loss: 2713.7288387305603\n",
      " Epoch 450/1000; Batch 1/1: [===========] - loss: 2712.2146908716963\n",
      " Epoch 451/1000; Batch 1/1: [===========] - loss: 2710.6986633591832\n",
      " Epoch 452/1000; Batch 1/1: [===========] - loss: 2709.180725914691\n",
      " Epoch 453/1000; Batch 1/1: [===========] - loss: 2707.6609001530323\n",
      " Epoch 454/1000; Batch 1/1: [===========] - loss: 2706.139124377778\n",
      " Epoch 455/1000; Batch 1/1: [===========] - loss: 2704.6154711623044\n",
      " Epoch 456/1000; Batch 1/1: [===========] - loss: 2703.0899097565816\n",
      " Epoch 457/1000; Batch 1/1: [===========] - loss: 2701.562351638561\n",
      " Epoch 458/1000; Batch 1/1: [===========] - loss: 2700.032757944557\n",
      " Epoch 459/1000; Batch 1/1: [===========] - loss: 2698.5011213830826\n",
      " Epoch 460/1000; Batch 1/1: [===========] - loss: 2696.9674599995915\n",
      " Epoch 461/1000; Batch 1/1: [===========] - loss: 2695.4316952449394\n",
      " Epoch 462/1000; Batch 1/1: [===========] - loss: 2693.8938042104664\n",
      " Epoch 463/1000; Batch 1/1: [===========] - loss: 2692.353768718012\n",
      " Epoch 464/1000; Batch 1/1: [===========] - loss: 2690.811584352535\n",
      " Epoch 465/1000; Batch 1/1: [===========] - loss: 2689.267259823461\n",
      " Epoch 466/1000; Batch 1/1: [===========] - loss: 2687.720774293753\n",
      " Epoch 467/1000; Batch 1/1: [===========] - loss: 2686.172156607163\n",
      " Epoch 468/1000; Batch 1/1: [===========] - loss: 2684.6214087513786\n",
      " Epoch 469/1000; Batch 1/1: [===========] - loss: 2683.0684066455965\n",
      " Epoch 470/1000; Batch 1/1: [===========] - loss: 2681.51314397218\n",
      " Epoch 471/1000; Batch 1/1: [===========] - loss: 2679.9555629657443\n",
      " Epoch 472/1000; Batch 1/1: [===========] - loss: 2678.3956402260633\n",
      " Epoch 473/1000; Batch 1/1: [===========] - loss: 2676.833450066692\n",
      " Epoch 474/1000; Batch 1/1: [===========] - loss: 2675.2688978704778\n",
      " Epoch 475/1000; Batch 1/1: [===========] - loss: 2673.7019640762946\n",
      " Epoch 476/1000; Batch 1/1: [===========] - loss: 2672.1327083159513\n",
      " Epoch 477/1000; Batch 1/1: [===========] - loss: 2670.5610323711276\n",
      " Epoch 478/1000; Batch 1/1: [===========] - loss: 2668.986937933078\n",
      " Epoch 479/1000; Batch 1/1: [===========] - loss: 2667.4103726982516\n",
      " Epoch 480/1000; Batch 1/1: [===========] - loss: 2665.831237580883\n",
      " Epoch 481/1000; Batch 1/1: [===========] - loss: 2664.24952660848\n",
      " Epoch 482/1000; Batch 1/1: [===========] - loss: 2662.6652904311372\n",
      " Epoch 483/1000; Batch 1/1: [===========] - loss: 2661.078611334333\n",
      " Epoch 484/1000; Batch 1/1: [===========] - loss: 2659.48938557548\n",
      " Epoch 485/1000; Batch 1/1: [===========] - loss: 2657.8974320333973\n",
      " Epoch 486/1000; Batch 1/1: [===========] - loss: 2656.3028470443787\n",
      " Epoch 487/1000; Batch 1/1: [===========] - loss: 2654.705577255462\n",
      " Epoch 488/1000; Batch 1/1: [===========] - loss: 2653.105675121938\n",
      " Epoch 489/1000; Batch 1/1: [===========] - loss: 2651.5030567689864\n",
      " Epoch 490/1000; Batch 1/1: [===========] - loss: 2649.897604652431\n",
      " Epoch 491/1000; Batch 1/1: [===========] - loss: 2648.28912243675\n",
      " Epoch 492/1000; Batch 1/1: [===========] - loss: 2646.6777060938593\n",
      " Epoch 493/1000; Batch 1/1: [===========] - loss: 2645.063471220082\n",
      " Epoch 494/1000; Batch 1/1: [===========] - loss: 2643.4464516485386\n",
      " Epoch 495/1000; Batch 1/1: [===========] - loss: 2641.82683922037\n",
      " Epoch 496/1000; Batch 1/1: [===========] - loss: 2640.20456043455\n",
      " Epoch 497/1000; Batch 1/1: [===========] - loss: 2638.5795366879956\n",
      " Epoch 498/1000; Batch 1/1: [===========] - loss: 2636.951831468048\n",
      " Epoch 499/1000; Batch 1/1: [===========] - loss: 2635.3213997386756\n",
      " Epoch 500/1000; Batch 1/1: [===========] - loss: 2633.6882098797873\n",
      " Epoch 501/1000; Batch 1/1: [===========] - loss: 2632.052542787774\n",
      " Epoch 502/1000; Batch 1/1: [===========] - loss: 2630.414032865895\n",
      " Epoch 503/1000; Batch 1/1: [===========] - loss: 2628.772839069149\n",
      " Epoch 504/1000; Batch 1/1: [===========] - loss: 2627.1287594517185\n",
      " Epoch 505/1000; Batch 1/1: [===========] - loss: 2625.4817212435296\n",
      " Epoch 506/1000; Batch 1/1: [===========] - loss: 2623.8315816372588\n",
      " Epoch 507/1000; Batch 1/1: [===========] - loss: 2622.1780702519764\n",
      " Epoch 508/1000; Batch 1/1: [===========] - loss: 2620.5213592447144\n",
      " Epoch 509/1000; Batch 1/1: [===========] - loss: 2618.8617470905097\n",
      " Epoch 510/1000; Batch 1/1: [===========] - loss: 2617.1989665057845\n",
      " Epoch 511/1000; Batch 1/1: [===========] - loss: 2615.5332620388567\n",
      " Epoch 512/1000; Batch 1/1: [===========] - loss: 2613.864296884901\n",
      " Epoch 513/1000; Batch 1/1: [===========] - loss: 2612.1919206944904\n",
      " Epoch 514/1000; Batch 1/1: [===========] - loss: 2610.516356709971\n",
      " Epoch 515/1000; Batch 1/1: [===========] - loss: 2608.8374676266167\n",
      " Epoch 516/1000; Batch 1/1: [===========] - loss: 2607.155166756373\n",
      " Epoch 517/1000; Batch 1/1: [===========] - loss: 2605.4701996070967\n",
      " Epoch 518/1000; Batch 1/1: [===========] - loss: 2603.7819115249313\n",
      " Epoch 519/1000; Batch 1/1: [===========] - loss: 2602.090941949184\n",
      " Epoch 520/1000; Batch 1/1: [===========] - loss: 2600.397426021709\n",
      " Epoch 521/1000; Batch 1/1: [===========] - loss: 2598.7011858331243\n",
      " Epoch 522/1000; Batch 1/1: [===========] - loss: 2597.0017733135205\n",
      " Epoch 523/1000; Batch 1/1: [===========] - loss: 2595.2990572902945\n",
      " Epoch 524/1000; Batch 1/1: [===========] - loss: 2593.5936363330006\n",
      " Epoch 525/1000; Batch 1/1: [===========] - loss: 2591.8851217720085\n",
      " Epoch 526/1000; Batch 1/1: [===========] - loss: 2590.173445333647\n",
      " Epoch 527/1000; Batch 1/1: [===========] - loss: 2588.458576613216\n",
      " Epoch 528/1000; Batch 1/1: [===========] - loss: 2586.7407222314796\n",
      " Epoch 529/1000; Batch 1/1: [===========] - loss: 2585.0195204427023\n",
      " Epoch 530/1000; Batch 1/1: [===========] - loss: 2583.2949650734295\n",
      " Epoch 531/1000; Batch 1/1: [===========] - loss: 2581.5670729421754\n",
      " Epoch 532/1000; Batch 1/1: [===========] - loss: 2579.835842073424\n",
      " Epoch 533/1000; Batch 1/1: [===========] - loss: 2578.1012288470024\n",
      " Epoch 534/1000; Batch 1/1: [===========] - loss: 2576.3633480236927\n",
      " Epoch 535/1000; Batch 1/1: [===========] - loss: 2574.6221041920194\n",
      " Epoch 536/1000; Batch 1/1: [===========] - loss: 2572.8775137189455\n",
      " Epoch 537/1000; Batch 1/1: [===========] - loss: 2571.129534112325\n",
      " Epoch 538/1000; Batch 1/1: [===========] - loss: 2569.378106913086\n",
      " Epoch 539/1000; Batch 1/1: [===========] - loss: 2567.6231548111477\n",
      " Epoch 540/1000; Batch 1/1: [===========] - loss: 2565.8646389409996\n",
      " Epoch 541/1000; Batch 1/1: [===========] - loss: 2564.1026490698077\n",
      " Epoch 542/1000; Batch 1/1: [===========] - loss: 2562.337160922195\n",
      " Epoch 543/1000; Batch 1/1: [===========] - loss: 2560.56808933055\n",
      " Epoch 544/1000; Batch 1/1: [===========] - loss: 2558.7955149042587\n",
      " Epoch 545/1000; Batch 1/1: [===========] - loss: 2557.0193780018753\n",
      " Epoch 546/1000; Batch 1/1: [===========] - loss: 2555.239662105084\n",
      " Epoch 547/1000; Batch 1/1: [===========] - loss: 2553.4562713933897\n",
      " Epoch 548/1000; Batch 1/1: [===========] - loss: 2551.669272772435\n",
      " Epoch 549/1000; Batch 1/1: [===========] - loss: 2549.878573417098\n",
      " Epoch 550/1000; Batch 1/1: [===========] - loss: 2548.0842359099847\n",
      " Epoch 551/1000; Batch 1/1: [===========] - loss: 2546.286227642096\n",
      " Epoch 552/1000; Batch 1/1: [===========] - loss: 2544.484459852577\n",
      " Epoch 553/1000; Batch 1/1: [===========] - loss: 2542.6789017711767\n",
      " Epoch 554/1000; Batch 1/1: [===========] - loss: 2540.869566903587\n",
      " Epoch 555/1000; Batch 1/1: [===========] - loss: 2539.056535695898\n",
      " Epoch 556/1000; Batch 1/1: [===========] - loss: 2537.239669012825\n",
      " Epoch 557/1000; Batch 1/1: [===========] - loss: 2535.4189405804773\n",
      " Epoch 558/1000; Batch 1/1: [===========] - loss: 2533.594357623842\n",
      " Epoch 559/1000; Batch 1/1: [===========] - loss: 2531.7659479037056\n",
      " Epoch 560/1000; Batch 1/1: [===========] - loss: 2529.933678701812\n",
      " Epoch 561/1000; Batch 1/1: [===========] - loss: 2528.0975522860026\n",
      " Epoch 562/1000; Batch 1/1: [===========] - loss: 2526.2575464139754\n",
      " Epoch 563/1000; Batch 1/1: [===========] - loss: 2524.413617826718\n",
      " Epoch 564/1000; Batch 1/1: [===========] - loss: 2522.565714890903\n",
      " Epoch 565/1000; Batch 1/1: [===========] - loss: 2520.713811492157\n",
      " Epoch 566/1000; Batch 1/1: [===========] - loss: 2518.857913175977\n",
      " Epoch 567/1000; Batch 1/1: [===========] - loss: 2516.9980496350668\n",
      " Epoch 568/1000; Batch 1/1: [===========] - loss: 2515.134180089492\n",
      " Epoch 569/1000; Batch 1/1: [===========] - loss: 2513.2663308683073\n",
      " Epoch 570/1000; Batch 1/1: [===========] - loss: 2511.39453929513\n",
      " Epoch 571/1000; Batch 1/1: [===========] - loss: 2509.518698913959\n",
      " Epoch 572/1000; Batch 1/1: [===========] - loss: 2507.6388300793674\n",
      " Epoch 573/1000; Batch 1/1: [===========] - loss: 2505.7548718006383\n",
      " Epoch 574/1000; Batch 1/1: [===========] - loss: 2503.866936472022\n",
      " Epoch 575/1000; Batch 1/1: [===========] - loss: 2501.975074390779\n",
      " Epoch 576/1000; Batch 1/1: [===========] - loss: 2500.0790840257496\n",
      " Epoch 577/1000; Batch 1/1: [===========] - loss: 2498.178923802224\n",
      " Epoch 578/1000; Batch 1/1: [===========] - loss: 2496.2746200644156\n",
      " Epoch 579/1000; Batch 1/1: [===========] - loss: 2494.3661090842834\n",
      " Epoch 580/1000; Batch 1/1: [===========] - loss: 2492.4534103778387\n",
      " Epoch 581/1000; Batch 1/1: [===========] - loss: 2490.536489366392\n",
      " Epoch 582/1000; Batch 1/1: [===========] - loss: 2488.615358279466\n",
      " Epoch 583/1000; Batch 1/1: [===========] - loss: 2486.690037605047\n",
      " Epoch 584/1000; Batch 1/1: [===========] - loss: 2484.760509679626\n",
      " Epoch 585/1000; Batch 1/1: [===========] - loss: 2482.8267235437243\n",
      " Epoch 586/1000; Batch 1/1: [===========] - loss: 2480.888766607048\n",
      " Epoch 587/1000; Batch 1/1: [===========] - loss: 2478.9466113888816\n",
      " Epoch 588/1000; Batch 1/1: [===========] - loss: 2477.0002007416315\n",
      " Epoch 589/1000; Batch 1/1: [===========] - loss: 2475.049544263618\n",
      " Epoch 590/1000; Batch 1/1: [===========] - loss: 2473.094678657963\n",
      " Epoch 591/1000; Batch 1/1: [===========] - loss: 2471.13550591311\n",
      " Epoch 592/1000; Batch 1/1: [===========] - loss: 2469.172107307514\n",
      " Epoch 593/1000; Batch 1/1: [===========] - loss: 2467.2043834185174\n",
      " Epoch 594/1000; Batch 1/1: [===========] - loss: 2465.232337974425\n",
      " Epoch 595/1000; Batch 1/1: [===========] - loss: 2463.2559805805636\n",
      " Epoch 596/1000; Batch 1/1: [===========] - loss: 2461.2753086580024\n",
      " Epoch 597/1000; Batch 1/1: [===========] - loss: 2459.29032108306\n",
      " Epoch 598/1000; Batch 1/1: [===========] - loss: 2457.300971792292\n",
      " Epoch 599/1000; Batch 1/1: [===========] - loss: 2455.307268923104\n",
      " Epoch 600/1000; Batch 1/1: [===========] - loss: 2453.3091371400483\n",
      " Epoch 601/1000; Batch 1/1: [===========] - loss: 2451.30656839477\n",
      " Epoch 602/1000; Batch 1/1: [===========] - loss: 2449.2996106441396\n",
      " Epoch 603/1000; Batch 1/1: [===========] - loss: 2447.2882922571193\n",
      " Epoch 604/1000; Batch 1/1: [===========] - loss: 2445.272566303198\n",
      " Epoch 605/1000; Batch 1/1: [===========] - loss: 2443.252404370058\n",
      " Epoch 606/1000; Batch 1/1: [===========] - loss: 2441.2277739930723\n",
      " Epoch 607/1000; Batch 1/1: [===========] - loss: 2439.198642740136\n",
      " Epoch 608/1000; Batch 1/1: [===========] - loss: 2437.164993857217\n",
      " Epoch 609/1000; Batch 1/1: [===========] - loss: 2435.1269035946034\n",
      " Epoch 610/1000; Batch 1/1: [===========] - loss: 2433.084307848367\n",
      " Epoch 611/1000; Batch 1/1: [===========] - loss: 2431.037243918565\n",
      " Epoch 612/1000; Batch 1/1: [===========] - loss: 2428.9857181306556\n",
      " Epoch 613/1000; Batch 1/1: [===========] - loss: 2426.9296652373964\n",
      " Epoch 614/1000; Batch 1/1: [===========] - loss: 2424.869105916051\n",
      " Epoch 615/1000; Batch 1/1: [===========] - loss: 2422.804040116167\n",
      " Epoch 616/1000; Batch 1/1: [===========] - loss: 2420.7345388988656\n",
      " Epoch 617/1000; Batch 1/1: [===========] - loss: 2418.660538487804\n",
      " Epoch 618/1000; Batch 1/1: [===========] - loss: 2416.582047953554\n",
      " Epoch 619/1000; Batch 1/1: [===========] - loss: 2414.499137701381\n",
      " Epoch 620/1000; Batch 1/1: [===========] - loss: 2412.411709105662\n",
      " Epoch 621/1000; Batch 1/1: [===========] - loss: 2410.3197230478418\n",
      " Epoch 622/1000; Batch 1/1: [===========] - loss: 2408.223252488171\n",
      " Epoch 623/1000; Batch 1/1: [===========] - loss: 2406.1222694615362\n",
      " Epoch 624/1000; Batch 1/1: [===========] - loss: 2404.016777731389\n",
      " Epoch 625/1000; Batch 1/1: [===========] - loss: 2401.9068097932172\n",
      " Epoch 626/1000; Batch 1/1: [===========] - loss: 2399.7923508330136\n",
      " Epoch 627/1000; Batch 1/1: [===========] - loss: 2397.6733658547055\n",
      " Epoch 628/1000; Batch 1/1: [===========] - loss: 2395.549852183524\n",
      " Epoch 629/1000; Batch 1/1: [===========] - loss: 2393.4218012041833\n",
      " Epoch 630/1000; Batch 1/1: [===========] - loss: 2391.289183668995\n",
      " Epoch 631/1000; Batch 1/1: [===========] - loss: 2389.152049275949\n",
      " Epoch 632/1000; Batch 1/1: [===========] - loss: 2387.010392764651\n",
      " Epoch 633/1000; Batch 1/1: [===========] - loss: 2384.8642080331465\n",
      " Epoch 634/1000; Batch 1/1: [===========] - loss: 2382.713496695139\n",
      " Epoch 635/1000; Batch 1/1: [===========] - loss: 2380.558268814575\n",
      " Epoch 636/1000; Batch 1/1: [===========] - loss: 2378.3985735426786\n",
      " Epoch 637/1000; Batch 1/1: [===========] - loss: 2376.2343458250866\n",
      " Epoch 638/1000; Batch 1/1: [===========] - loss: 2374.0656388293924\n",
      " Epoch 639/1000; Batch 1/1: [===========] - loss: 2371.8924112651675\n",
      " Epoch 640/1000; Batch 1/1: [===========] - loss: 2369.714686584162\n",
      " Epoch 641/1000; Batch 1/1: [===========] - loss: 2367.532475056644\n",
      " Epoch 642/1000; Batch 1/1: [===========] - loss: 2365.345772094132\n",
      " Epoch 643/1000; Batch 1/1: [===========] - loss: 2363.1545991069875\n",
      " Epoch 644/1000; Batch 1/1: [===========] - loss: 2360.958951408663\n",
      " Epoch 645/1000; Batch 1/1: [===========] - loss: 2358.758849110528\n",
      " Epoch 646/1000; Batch 1/1: [===========] - loss: 2356.554308141412\n",
      " Epoch 647/1000; Batch 1/1: [===========] - loss: 2354.3453129374598\n",
      " Epoch 648/1000; Batch 1/1: [===========] - loss: 2352.131864632332\n",
      " Epoch 649/1000; Batch 1/1: [===========] - loss: 2349.9140226761515\n",
      " Epoch 650/1000; Batch 1/1: [===========] - loss: 2347.691743743597\n",
      " Epoch 651/1000; Batch 1/1: [===========] - loss: 2345.4650185142964\n",
      " Epoch 652/1000; Batch 1/1: [===========] - loss: 2343.2338961159444\n",
      " Epoch 653/1000; Batch 1/1: [===========] - loss: 2340.9983925333627\n",
      " Epoch 654/1000; Batch 1/1: [===========] - loss: 2338.7585251806677\n",
      " Epoch 655/1000; Batch 1/1: [===========] - loss: 2336.5143241823735\n",
      " Epoch 656/1000; Batch 1/1: [===========] - loss: 2334.2657776014835\n",
      " Epoch 657/1000; Batch 1/1: [===========] - loss: 2332.012876253874\n",
      " Epoch 658/1000; Batch 1/1: [===========] - loss: 2329.755645892657\n",
      " Epoch 659/1000; Batch 1/1: [===========] - loss: 2327.4941154470152\n",
      " Epoch 660/1000; Batch 1/1: [===========] - loss: 2325.228300824403\n",
      " Epoch 661/1000; Batch 1/1: [===========] - loss: 2322.9582021069273\n",
      " Epoch 662/1000; Batch 1/1: [===========] - loss: 2320.683866989362\n",
      " Epoch 663/1000; Batch 1/1: [===========] - loss: 2318.4053018887053\n",
      " Epoch 664/1000; Batch 1/1: [===========] - loss: 2316.122511856932\n",
      " Epoch 665/1000; Batch 1/1: [===========] - loss: 2313.83547389062\n",
      " Epoch 666/1000; Batch 1/1: [===========] - loss: 2311.544260270621\n",
      " Epoch 667/1000; Batch 1/1: [===========] - loss: 2309.248878304731\n",
      " Epoch 668/1000; Batch 1/1: [===========] - loss: 2306.9493573309783\n",
      " Epoch 669/1000; Batch 1/1: [===========] - loss: 2304.645742330803\n",
      " Epoch 670/1000; Batch 1/1: [===========] - loss: 2302.33804909599\n",
      " Epoch 671/1000; Batch 1/1: [===========] - loss: 2300.02627760912\n",
      " Epoch 672/1000; Batch 1/1: [===========] - loss: 2297.710470514963\n",
      " Epoch 673/1000; Batch 1/1: [===========] - loss: 2295.390661671146\n",
      " Epoch 674/1000; Batch 1/1: [===========] - loss: 2293.0668763297545\n",
      " Epoch 675/1000; Batch 1/1: [===========] - loss: 2290.739303802364\n",
      " Epoch 676/1000; Batch 1/1: [===========] - loss: 2288.407812389759\n",
      " Epoch 677/1000; Batch 1/1: [===========] - loss: 2286.072434873215\n",
      " Epoch 678/1000; Batch 1/1: [===========] - loss: 2283.7332188731793\n",
      " Epoch 679/1000; Batch 1/1: [===========] - loss: 2281.3902014294727\n",
      " Epoch 680/1000; Batch 1/1: [===========] - loss: 2279.043366378335\n",
      " Epoch 681/1000; Batch 1/1: [===========] - loss: 2276.692751655609\n",
      " Epoch 682/1000; Batch 1/1: [===========] - loss: 2274.338415874286\n",
      " Epoch 683/1000; Batch 1/1: [===========] - loss: 2271.980359009977\n",
      " Epoch 684/1000; Batch 1/1: [===========] - loss: 2269.6186139716765\n",
      " Epoch 685/1000; Batch 1/1: [===========] - loss: 2267.2532109885333\n",
      " Epoch 686/1000; Batch 1/1: [===========] - loss: 2264.884198172424\n",
      " Epoch 687/1000; Batch 1/1: [===========] - loss: 2262.5115993485356\n",
      " Epoch 688/1000; Batch 1/1: [===========] - loss: 2260.1355000821154\n",
      " Epoch 689/1000; Batch 1/1: [===========] - loss: 2257.755929762651\n",
      " Epoch 690/1000; Batch 1/1: [===========] - loss: 2255.3728806618406\n",
      " Epoch 691/1000; Batch 1/1: [===========] - loss: 2252.986398969189\n",
      " Epoch 692/1000; Batch 1/1: [===========] - loss: 2250.596513958815\n",
      " Epoch 693/1000; Batch 1/1: [===========] - loss: 2248.203244625415\n",
      " Epoch 694/1000; Batch 1/1: [===========] - loss: 2245.80665543743\n",
      " Epoch 695/1000; Batch 1/1: [===========] - loss: 2243.4068091285753\n",
      " Epoch 696/1000; Batch 1/1: [===========] - loss: 2241.003783903597\n",
      " Epoch 697/1000; Batch 1/1: [===========] - loss: 2238.597575630183\n",
      " Epoch 698/1000; Batch 1/1: [===========] - loss: 2236.1882235436865\n",
      " Epoch 699/1000; Batch 1/1: [===========] - loss: 2233.775767315554\n",
      " Epoch 700/1000; Batch 1/1: [===========] - loss: 2231.3602622169715\n",
      " Epoch 701/1000; Batch 1/1: [===========] - loss: 2228.941756891385\n",
      " Epoch 702/1000; Batch 1/1: [===========] - loss: 2226.520291390583\n",
      " Epoch 703/1000; Batch 1/1: [===========] - loss: 2224.095917254084\n",
      " Epoch 704/1000; Batch 1/1: [===========] - loss: 2221.6686721478445\n",
      " Epoch 705/1000; Batch 1/1: [===========] - loss: 2219.2386187951893\n",
      " Epoch 706/1000; Batch 1/1: [===========] - loss: 2216.8057940047856\n",
      " Epoch 707/1000; Batch 1/1: [===========] - loss: 2214.3702885471553\n",
      " Epoch 708/1000; Batch 1/1: [===========] - loss: 2211.932145549767\n",
      " Epoch 709/1000; Batch 1/1: [===========] - loss: 2209.4914060963583\n",
      " Epoch 710/1000; Batch 1/1: [===========] - loss: 2207.048119327623\n",
      " Epoch 711/1000; Batch 1/1: [===========] - loss: 2204.6023594438793\n",
      " Epoch 712/1000; Batch 1/1: [===========] - loss: 2202.1541596334596\n",
      " Epoch 713/1000; Batch 1/1: [===========] - loss: 2199.7035860639276\n",
      " Epoch 714/1000; Batch 1/1: [===========] - loss: 2197.2506957231194\n",
      " Epoch 715/1000; Batch 1/1: [===========] - loss: 2194.795547541093\n",
      " Epoch 716/1000; Batch 1/1: [===========] - loss: 2192.3381820650566\n",
      " Epoch 717/1000; Batch 1/1: [===========] - loss: 2189.8786621143377\n",
      " Epoch 718/1000; Batch 1/1: [===========] - loss: 2187.417060266614\n",
      " Epoch 719/1000; Batch 1/1: [===========] - loss: 2184.953430334261\n",
      " Epoch 720/1000; Batch 1/1: [===========] - loss: 2182.487828523276\n",
      " Epoch 721/1000; Batch 1/1: [===========] - loss: 2180.0203182210466\n",
      " Epoch 722/1000; Batch 1/1: [===========] - loss: 2177.5509636307806\n",
      " Epoch 723/1000; Batch 1/1: [===========] - loss: 2175.0798366992863\n",
      " Epoch 724/1000; Batch 1/1: [===========] - loss: 2172.607003180266\n",
      " Epoch 725/1000; Batch 1/1: [===========] - loss: 2170.132512111134\n",
      " Epoch 726/1000; Batch 1/1: [===========] - loss: 2167.6564394581105\n",
      " Epoch 727/1000; Batch 1/1: [===========] - loss: 2165.178870045004\n",
      " Epoch 728/1000; Batch 1/1: [===========] - loss: 2162.6998494246413\n",
      " Epoch 729/1000; Batch 1/1: [===========] - loss: 2160.2194551778816\n",
      " Epoch 730/1000; Batch 1/1: [===========] - loss: 2157.7377531773104\n",
      " Epoch 731/1000; Batch 1/1: [===========] - loss: 2155.254814870714\n",
      " Epoch 732/1000; Batch 1/1: [===========] - loss: 2152.7707152857174\n",
      " Epoch 733/1000; Batch 1/1: [===========] - loss: 2150.285521357134\n",
      " Epoch 734/1000; Batch 1/1: [===========] - loss: 2147.799308438599\n",
      " Epoch 735/1000; Batch 1/1: [===========] - loss: 2145.3121440488885\n",
      " Epoch 736/1000; Batch 1/1: [===========] - loss: 2142.824117585587\n",
      " Epoch 737/1000; Batch 1/1: [===========] - loss: 2140.3352843385715\n",
      " Epoch 738/1000; Batch 1/1: [===========] - loss: 2137.845731807\n",
      " Epoch 739/1000; Batch 1/1: [===========] - loss: 2135.3555358711387\n",
      " Epoch 740/1000; Batch 1/1: [===========] - loss: 2132.8648192678183\n",
      " Epoch 741/1000; Batch 1/1: [===========] - loss: 2130.37360657917\n",
      " Epoch 742/1000; Batch 1/1: [===========] - loss: 2127.881991645529\n",
      " Epoch 743/1000; Batch 1/1: [===========] - loss: 2125.3900439328654\n",
      " Epoch 744/1000; Batch 1/1: [===========] - loss: 2122.8978502207756\n",
      " Epoch 745/1000; Batch 1/1: [===========] - loss: 2120.4054919960868\n",
      " Epoch 746/1000; Batch 1/1: [===========] - loss: 2117.91305928812\n",
      " Epoch 747/1000; Batch 1/1: [===========] - loss: 2115.4206263595356\n",
      " Epoch 748/1000; Batch 1/1: [===========] - loss: 2112.9282858268652\n",
      " Epoch 749/1000; Batch 1/1: [===========] - loss: 2110.436120036936\n",
      " Epoch 750/1000; Batch 1/1: [===========] - loss: 2107.944201613535\n",
      " Epoch 751/1000; Batch 1/1: [===========] - loss: 2105.452618058706\n",
      " Epoch 752/1000; Batch 1/1: [===========] - loss: 2102.961456931889\n",
      " Epoch 753/1000; Batch 1/1: [===========] - loss: 2100.4708010425247\n",
      " Epoch 754/1000; Batch 1/1: [===========] - loss: 2097.9807369194023\n",
      " Epoch 755/1000; Batch 1/1: [===========] - loss: 2095.4913510601605\n",
      " Epoch 756/1000; Batch 1/1: [===========] - loss: 2093.002730816195\n",
      " Epoch 757/1000; Batch 1/1: [===========] - loss: 2090.514962151396\n",
      " Epoch 758/1000; Batch 1/1: [===========] - loss: 2088.0281345461376\n",
      " Epoch 759/1000; Batch 1/1: [===========] - loss: 2085.5423369489117\n",
      " Epoch 760/1000; Batch 1/1: [===========] - loss: 2083.0576559964993\n",
      " Epoch 761/1000; Batch 1/1: [===========] - loss: 2080.5741789491744\n",
      " Epoch 762/1000; Batch 1/1: [===========] - loss: 2078.09199616291\n",
      " Epoch 763/1000; Batch 1/1: [===========] - loss: 2075.6112018936974\n",
      " Epoch 764/1000; Batch 1/1: [===========] - loss: 2073.131884456291\n",
      " Epoch 765/1000; Batch 1/1: [===========] - loss: 2070.654138709245\n",
      " Epoch 766/1000; Batch 1/1: [===========] - loss: 2068.1780641528353\n",
      " Epoch 767/1000; Batch 1/1: [===========] - loss: 2065.703748399458\n",
      " Epoch 768/1000; Batch 1/1: [===========] - loss: 2063.2312777809807\n",
      " Epoch 769/1000; Batch 1/1: [===========] - loss: 2060.76074237927\n",
      " Epoch 770/1000; Batch 1/1: [===========] - loss: 2058.292243097717\n",
      " Epoch 771/1000; Batch 1/1: [===========] - loss: 2055.8258719851747\n",
      " Epoch 772/1000; Batch 1/1: [===========] - loss: 2053.3617198908028\n",
      " Epoch 773/1000; Batch 1/1: [===========] - loss: 2050.8998795737057\n",
      " Epoch 774/1000; Batch 1/1: [===========] - loss: 2048.440444855032\n",
      " Epoch 775/1000; Batch 1/1: [===========] - loss: 2045.983505217367\n",
      " Epoch 776/1000; Batch 1/1: [===========] - loss: 2043.5291603253802\n",
      " Epoch 777/1000; Batch 1/1: [===========] - loss: 2041.0775024734178\n",
      " Epoch 778/1000; Batch 1/1: [===========] - loss: 2038.6286213811366\n",
      " Epoch 779/1000; Batch 1/1: [===========] - loss: 2036.1826196754555\n",
      " Epoch 780/1000; Batch 1/1: [===========] - loss: 2033.7395949492018\n",
      " Epoch 781/1000; Batch 1/1: [===========] - loss: 2031.2993362750965\n",
      " Epoch 782/1000; Batch 1/1: [===========] - loss: 2028.861644423127\n",
      " Epoch 783/1000; Batch 1/1: [===========] - loss: 2026.427193355484\n",
      " Epoch 784/1000; Batch 1/1: [===========] - loss: 2023.9960855097174\n",
      " Epoch 785/1000; Batch 1/1: [===========] - loss: 2021.5684148586683\n",
      " Epoch 786/1000; Batch 1/1: [===========] - loss: 2019.144278078634\n",
      " Epoch 787/1000; Batch 1/1: [===========] - loss: 2016.7237716236104\n",
      " Epoch 788/1000; Batch 1/1: [===========] - loss: 2014.3069967477222\n",
      " Epoch 789/1000; Batch 1/1: [===========] - loss: 2011.894271207132\n",
      " Epoch 790/1000; Batch 1/1: [===========] - loss: 2009.4855664405902\n",
      " Epoch 791/1000; Batch 1/1: [===========] - loss: 2007.0807199042777\n",
      " Epoch 792/1000; Batch 1/1: [===========] - loss: 2004.679992603188\n",
      " Epoch 793/1000; Batch 1/1: [===========] - loss: 2002.2840568068816\n",
      " Epoch 794/1000; Batch 1/1: [===========] - loss: 1999.89240116842\n",
      " Epoch 795/1000; Batch 1/1: [===========] - loss: 1997.5053452421985\n",
      " Epoch 796/1000; Batch 1/1: [===========] - loss: 1995.1238734878198\n",
      " Epoch 797/1000; Batch 1/1: [===========] - loss: 1992.7469574164722\n",
      " Epoch 798/1000; Batch 1/1: [===========] - loss: 1990.3746911866097\n",
      " Epoch 799/1000; Batch 1/1: [===========] - loss: 1988.0071801281533\n",
      " Epoch 800/1000; Batch 1/1: [===========] - loss: 1985.6444942484193\n",
      " Epoch 801/1000; Batch 1/1: [===========] - loss: 1983.2867294822195\n",
      " Epoch 802/1000; Batch 1/1: [===========] - loss: 1980.9339798375945\n",
      " Epoch 803/1000; Batch 1/1: [===========] - loss: 1978.5868242535219\n",
      " Epoch 804/1000; Batch 1/1: [===========] - loss: 1976.2458042028961\n",
      " Epoch 805/1000; Batch 1/1: [===========] - loss: 1973.91029032401\n",
      " Epoch 806/1000; Batch 1/1: [===========] - loss: 1971.5801253236853\n",
      " Epoch 807/1000; Batch 1/1: [===========] - loss: 1969.2554062051252\n",
      " Epoch 808/1000; Batch 1/1: [===========] - loss: 1966.9362098509046\n",
      " Epoch 809/1000; Batch 1/1: [===========] - loss: 1964.6222435303796\n",
      " Epoch 810/1000; Batch 1/1: [===========] - loss: 1962.3141549866066\n",
      " Epoch 811/1000; Batch 1/1: [===========] - loss: 1960.0127174654065\n",
      " Epoch 812/1000; Batch 1/1: [===========] - loss: 1957.717564080953\n",
      " Epoch 813/1000; Batch 1/1: [===========] - loss: 1955.428780014221\n",
      " Epoch 814/1000; Batch 1/1: [===========] - loss: 1953.1462944764564\n",
      " Epoch 815/1000; Batch 1/1: [===========] - loss: 1950.869918845961\n",
      " Epoch 816/1000; Batch 1/1: [===========] - loss: 1948.5997324387681\n",
      " Epoch 817/1000; Batch 1/1: [===========] - loss: 1946.3358279614931\n",
      " Epoch 818/1000; Batch 1/1: [===========] - loss: 1944.0783019590458\n",
      " Epoch 819/1000; Batch 1/1: [===========] - loss: 1941.8273663882937\n",
      " Epoch 820/1000; Batch 1/1: [===========] - loss: 1939.5834983315817\n",
      " Epoch 821/1000; Batch 1/1: [===========] - loss: 1937.3464924223422\n",
      " Epoch 822/1000; Batch 1/1: [===========] - loss: 1935.1161542593968\n",
      " Epoch 823/1000; Batch 1/1: [===========] - loss: 1932.8925633627707\n",
      " Epoch 824/1000; Batch 1/1: [===========] - loss: 1930.6758008454315\n",
      " Epoch 825/1000; Batch 1/1: [===========] - loss: 1928.4659414145128\n",
      " Epoch 826/1000; Batch 1/1: [===========] - loss: 1926.2630250047127\n",
      " Epoch 827/1000; Batch 1/1: [===========] - loss: 1924.0666573157812\n",
      " Epoch 828/1000; Batch 1/1: [===========] - loss: 1921.877531842187\n",
      " Epoch 829/1000; Batch 1/1: [===========] - loss: 1919.6958412159204\n",
      " Epoch 830/1000; Batch 1/1: [===========] - loss: 1917.5214649666848\n",
      " Epoch 831/1000; Batch 1/1: [===========] - loss: 1915.3543524732813\n",
      " Epoch 832/1000; Batch 1/1: [===========] - loss: 1913.1946238801318\n",
      " Epoch 833/1000; Batch 1/1: [===========] - loss: 1911.0424090705828\n",
      " Epoch 834/1000; Batch 1/1: [===========] - loss: 1908.8978664003191\n",
      " Epoch 835/1000; Batch 1/1: [===========] - loss: 1906.760927164401\n",
      " Epoch 836/1000; Batch 1/1: [===========] - loss: 1904.6320596522569\n",
      " Epoch 837/1000; Batch 1/1: [===========] - loss: 1902.5110837963416\n",
      " Epoch 838/1000; Batch 1/1: [===========] - loss: 1900.3977279947908\n",
      " Epoch 839/1000; Batch 1/1: [===========] - loss: 1898.292893814817\n",
      " Epoch 840/1000; Batch 1/1: [===========] - loss: 1896.1959977492143\n",
      " Epoch 841/1000; Batch 1/1: [===========] - loss: 1894.107010728099\n",
      " Epoch 842/1000; Batch 1/1: [===========] - loss: 1892.0258308082268\n",
      " Epoch 843/1000; Batch 1/1: [===========] - loss: 1889.9527439170156\n",
      " Epoch 844/1000; Batch 1/1: [===========] - loss: 1887.888192082015\n",
      " Epoch 845/1000; Batch 1/1: [===========] - loss: 1885.8318196610242\n",
      " Epoch 846/1000; Batch 1/1: [===========] - loss: 1883.7837053504325\n",
      " Epoch 847/1000; Batch 1/1: [===========] - loss: 1881.743940167694\n",
      " Epoch 848/1000; Batch 1/1: [===========] - loss: 1879.712509843278\n",
      " Epoch 849/1000; Batch 1/1: [===========] - loss: 1877.6895091101744\n",
      " Epoch 850/1000; Batch 1/1: [===========] - loss: 1875.6750372204112\n",
      " Epoch 851/1000; Batch 1/1: [===========] - loss: 1873.669191978157\n",
      " Epoch 852/1000; Batch 1/1: [===========] - loss: 1871.672149548852\n",
      " Epoch 853/1000; Batch 1/1: [===========] - loss: 1869.683626443281\n",
      " Epoch 854/1000; Batch 1/1: [===========] - loss: 1867.7035753884832\n",
      " Epoch 855/1000; Batch 1/1: [===========] - loss: 1865.7321670351453\n",
      " Epoch 856/1000; Batch 1/1: [===========] - loss: 1863.7694441830952\n",
      " Epoch 857/1000; Batch 1/1: [===========] - loss: 1861.8154496747213\n",
      " Epoch 858/1000; Batch 1/1: [===========] - loss: 1859.8702188518641\n",
      " Epoch 859/1000; Batch 1/1: [===========] - loss: 1857.933960588855\n",
      " Epoch 860/1000; Batch 1/1: [===========] - loss: 1856.0068866290474\n",
      " Epoch 861/1000; Batch 1/1: [===========] - loss: 1854.088801801351\n",
      " Epoch 862/1000; Batch 1/1: [===========] - loss: 1852.1795956657231\n",
      " Epoch 863/1000; Batch 1/1: [===========] - loss: 1850.2793311627743\n",
      " Epoch 864/1000; Batch 1/1: [===========] - loss: 1848.3880404272038\n",
      " Epoch 865/1000; Batch 1/1: [===========] - loss: 1846.505749980261\n",
      " Epoch 866/1000; Batch 1/1: [===========] - loss: 1844.6324518046526\n",
      " Epoch 867/1000; Batch 1/1: [===========] - loss: 1842.7680745723426\n",
      " Epoch 868/1000; Batch 1/1: [===========] - loss: 1840.9127299803063\n",
      " Epoch 869/1000; Batch 1/1: [===========] - loss: 1839.0663648758882\n",
      " Epoch 870/1000; Batch 1/1: [===========] - loss: 1837.2287151978844\n",
      " Epoch 871/1000; Batch 1/1: [===========] - loss: 1835.4002883793567\n",
      " Epoch 872/1000; Batch 1/1: [===========] - loss: 1833.5811141731826\n",
      " Epoch 873/1000; Batch 1/1: [===========] - loss: 1831.7710528965179\n",
      " Epoch 874/1000; Batch 1/1: [===========] - loss: 1829.9703532913231\n",
      " Epoch 875/1000; Batch 1/1: [===========] - loss: 1828.1783566318288\n",
      " Epoch 876/1000; Batch 1/1: [===========] - loss: 1826.395761744253\n",
      " Epoch 877/1000; Batch 1/1: [===========] - loss: 1824.6226193745838\n",
      " Epoch 878/1000; Batch 1/1: [===========] - loss: 1822.858693285702\n",
      " Epoch 879/1000; Batch 1/1: [===========] - loss: 1821.103993073546\n",
      " Epoch 880/1000; Batch 1/1: [===========] - loss: 1819.3585238614498\n",
      " Epoch 881/1000; Batch 1/1: [===========] - loss: 1817.622294365623\n",
      " Epoch 882/1000; Batch 1/1: [===========] - loss: 1815.8953122031244\n",
      " Epoch 883/1000; Batch 1/1: [===========] - loss: 1814.177607517192\n",
      " Epoch 884/1000; Batch 1/1: [===========] - loss: 1812.4691893086076\n",
      " Epoch 885/1000; Batch 1/1: [===========] - loss: 1810.7702339784098\n",
      " Epoch 886/1000; Batch 1/1: [===========] - loss: 1809.080810133463\n",
      " Epoch 887/1000; Batch 1/1: [===========] - loss: 1807.4006405274267\n",
      " Epoch 888/1000; Batch 1/1: [===========] - loss: 1805.7297215558679\n",
      " Epoch 889/1000; Batch 1/1: [===========] - loss: 1804.0679789486885\n",
      " Epoch 890/1000; Batch 1/1: [===========] - loss: 1802.415240901433\n",
      " Epoch 891/1000; Batch 1/1: [===========] - loss: 1800.7718240869367\n",
      " Epoch 892/1000; Batch 1/1: [===========] - loss: 1799.1378525782377\n",
      " Epoch 893/1000; Batch 1/1: [===========] - loss: 1797.5131395354833\n",
      " Epoch 894/1000; Batch 1/1: [===========] - loss: 1795.8980602837594\n",
      " Epoch 895/1000; Batch 1/1: [===========] - loss: 1794.2920775039215\n",
      " Epoch 896/1000; Batch 1/1: [===========] - loss: 1792.6953167553113\n",
      " Epoch 897/1000; Batch 1/1: [===========] - loss: 1791.1078671826915\n",
      " Epoch 898/1000; Batch 1/1: [===========] - loss: 1789.5295982480636\n",
      " Epoch 899/1000; Batch 1/1: [===========] - loss: 1787.9604594378288\n",
      " Epoch 900/1000; Batch 1/1: [===========] - loss: 1786.4004496577693\n",
      " Epoch 901/1000; Batch 1/1: [===========] - loss: 1784.849553465544\n",
      " Epoch 902/1000; Batch 1/1: [===========] - loss: 1783.3077705397723\n",
      " Epoch 903/1000; Batch 1/1: [===========] - loss: 1781.775144242151\n",
      " Epoch 904/1000; Batch 1/1: [===========] - loss: 1780.2515679280393\n",
      " Epoch 905/1000; Batch 1/1: [===========] - loss: 1778.737049065988\n",
      " Epoch 906/1000; Batch 1/1: [===========] - loss: 1777.2318117898321\n",
      " Epoch 907/1000; Batch 1/1: [===========] - loss: 1775.7356174323177\n",
      " Epoch 908/1000; Batch 1/1: [===========] - loss: 1774.2484793728852\n",
      " Epoch 909/1000; Batch 1/1: [===========] - loss: 1772.7702865355002\n",
      " Epoch 910/1000; Batch 1/1: [===========] - loss: 1771.3010003950892\n",
      " Epoch 911/1000; Batch 1/1: [===========] - loss: 1769.8406132447146\n",
      " Epoch 912/1000; Batch 1/1: [===========] - loss: 1768.3891539127696\n",
      " Epoch 913/1000; Batch 1/1: [===========] - loss: 1766.9465765505029\n",
      " Epoch 914/1000; Batch 1/1: [===========] - loss: 1765.5129586852026\n",
      " Epoch 915/1000; Batch 1/1: [===========] - loss: 1764.0881121425127\n",
      " Epoch 916/1000; Batch 1/1: [===========] - loss: 1762.672003194154\n",
      " Epoch 917/1000; Batch 1/1: [===========] - loss: 1761.263949306773\n",
      " Epoch 918/1000; Batch 1/1: [===========] - loss: 1759.8639157784585\n",
      " Epoch 919/1000; Batch 1/1: [===========] - loss: 1758.4725545981637\n",
      " Epoch 920/1000; Batch 1/1: [===========] - loss: 1757.0900087575133\n",
      " Epoch 921/1000; Batch 1/1: [===========] - loss: 1755.7162071145056\n",
      " Epoch 922/1000; Batch 1/1: [===========] - loss: 1754.3508528170253\n",
      " Epoch 923/1000; Batch 1/1: [===========] - loss: 1752.9940554363325\n",
      " Epoch 924/1000; Batch 1/1: [===========] - loss: 1751.6457385847598\n",
      " Epoch 925/1000; Batch 1/1: [===========] - loss: 1750.305864256693\n",
      " Epoch 926/1000; Batch 1/1: [===========] - loss: 1748.9739898005628\n",
      " Epoch 927/1000; Batch 1/1: [===========] - loss: 1747.6503909711187\n",
      " Epoch 928/1000; Batch 1/1: [===========] - loss: 1746.3351659253296\n",
      " Epoch 929/1000; Batch 1/1: [===========] - loss: 1745.0283786419366\n",
      " Epoch 930/1000; Batch 1/1: [===========] - loss: 1743.7296667245914\n",
      " Epoch 931/1000; Batch 1/1: [===========] - loss: 1742.4389893812959\n",
      " Epoch 932/1000; Batch 1/1: [===========] - loss: 1741.1562452758103\n",
      " Epoch 933/1000; Batch 1/1: [===========] - loss: 1739.8815573760362\n",
      " Epoch 934/1000; Batch 1/1: [===========] - loss: 1738.6150801315268\n",
      " Epoch 935/1000; Batch 1/1: [===========] - loss: 1737.356893664138\n",
      " Epoch 936/1000; Batch 1/1: [===========] - loss: 1736.1069646706192\n",
      " Epoch 937/1000; Batch 1/1: [===========] - loss: 1734.8650126684472\n",
      " Epoch 938/1000; Batch 1/1: [===========] - loss: 1733.6317789990005\n",
      " Epoch 939/1000; Batch 1/1: [===========] - loss: 1732.4063215057079\n",
      " Epoch 940/1000; Batch 1/1: [===========] - loss: 1731.1886124566533\n",
      " Epoch 941/1000; Batch 1/1: [===========] - loss: 1729.978776471844\n",
      " Epoch 942/1000; Batch 1/1: [===========] - loss: 1728.7768102562493\n",
      " Epoch 943/1000; Batch 1/1: [===========] - loss: 1727.582583039598\n",
      " Epoch 944/1000; Batch 1/1: [===========] - loss: 1726.3959435084814\n",
      " Epoch 945/1000; Batch 1/1: [===========] - loss: 1725.216836863876\n",
      " Epoch 946/1000; Batch 1/1: [===========] - loss: 1724.0457871903518\n",
      " Epoch 947/1000; Batch 1/1: [===========] - loss: 1722.8826025482183\n",
      " Epoch 948/1000; Batch 1/1: [===========] - loss: 1721.7264453734806\n",
      " Epoch 949/1000; Batch 1/1: [===========] - loss: 1720.5775953674452\n",
      " Epoch 950/1000; Batch 1/1: [===========] - loss: 1719.436209040134\n",
      " Epoch 951/1000; Batch 1/1: [===========] - loss: 1718.30223686871\n",
      " Epoch 952/1000; Batch 1/1: [===========] - loss: 1717.1755032366552\n",
      " Epoch 953/1000; Batch 1/1: [===========] - loss: 1716.0561558944014\n",
      " Epoch 954/1000; Batch 1/1: [===========] - loss: 1714.9443432880964\n",
      " Epoch 955/1000; Batch 1/1: [===========] - loss: 1713.8403440932705\n",
      " Epoch 956/1000; Batch 1/1: [===========] - loss: 1712.7433521847197\n",
      " Epoch 957/1000; Batch 1/1: [===========] - loss: 1711.6532006098887\n",
      " Epoch 958/1000; Batch 1/1: [===========] - loss: 1710.5697111388272\n",
      " Epoch 959/1000; Batch 1/1: [===========] - loss: 1709.4930341278434\n",
      " Epoch 960/1000; Batch 1/1: [===========] - loss: 1708.4232488381992\n",
      " Epoch 961/1000; Batch 1/1: [===========] - loss: 1707.360230232729\n",
      " Epoch 962/1000; Batch 1/1: [===========] - loss: 1706.3038990907223\n",
      " Epoch 963/1000; Batch 1/1: [===========] - loss: 1705.2541843229694\n",
      " Epoch 964/1000; Batch 1/1: [===========] - loss: 1704.2110293478815\n",
      " Epoch 965/1000; Batch 1/1: [===========] - loss: 1703.1744383031812\n",
      " Epoch 966/1000; Batch 1/1: [===========] - loss: 1702.144118576503\n",
      " Epoch 967/1000; Batch 1/1: [===========] - loss: 1701.1194651526525\n",
      " Epoch 968/1000; Batch 1/1: [===========] - loss: 1700.100856208734\n",
      " Epoch 969/1000; Batch 1/1: [===========] - loss: 1699.088019065247\n",
      " Epoch 970/1000; Batch 1/1: [===========] - loss: 1698.0815161850765\n",
      " Epoch 971/1000; Batch 1/1: [===========] - loss: 1697.0817332617085\n",
      " Epoch 972/1000; Batch 1/1: [===========] - loss: 1696.0880495371084\n",
      " Epoch 973/1000; Batch 1/1: [===========] - loss: 1695.099414260048\n",
      " Epoch 974/1000; Batch 1/1: [===========] - loss: 1694.1156530792111\n",
      " Epoch 975/1000; Batch 1/1: [===========] - loss: 1693.1379663370872\n",
      " Epoch 976/1000; Batch 1/1: [===========] - loss: 1692.1664567839605\n",
      " Epoch 977/1000; Batch 1/1: [===========] - loss: 1691.2009132659332\n",
      " Epoch 978/1000; Batch 1/1: [===========] - loss: 1690.2411341174675\n",
      " Epoch 979/1000; Batch 1/1: [===========] - loss: 1689.2871769117478\n",
      " Epoch 980/1000; Batch 1/1: [===========] - loss: 1688.339003655803\n",
      " Epoch 981/1000; Batch 1/1: [===========] - loss: 1687.3962633071685\n",
      " Epoch 982/1000; Batch 1/1: [===========] - loss: 1686.4589797671063\n",
      " Epoch 983/1000; Batch 1/1: [===========] - loss: 1685.5272123309023\n",
      " Epoch 984/1000; Batch 1/1: [===========] - loss: 1684.601077214668\n",
      " Epoch 985/1000; Batch 1/1: [===========] - loss: 1683.6804499281654\n",
      " Epoch 986/1000; Batch 1/1: [===========] - loss: 1682.7649938990344\n",
      " Epoch 987/1000; Batch 1/1: [===========] - loss: 1681.854589618074\n",
      " Epoch 988/1000; Batch 1/1: [===========] - loss: 1680.9478259637526\n",
      " Epoch 989/1000; Batch 1/1: [===========] - loss: 1680.04482713303\n",
      " Epoch 990/1000; Batch 1/1: [===========] - loss: 1679.1471602913225\n",
      " Epoch 991/1000; Batch 1/1: [===========] - loss: 1678.2540517929554\n",
      " Epoch 992/1000; Batch 1/1: [===========] - loss: 1677.3660462253101\n",
      " Epoch 993/1000; Batch 1/1: [===========] - loss: 1676.4829344932023\n",
      " Epoch 994/1000; Batch 1/1: [===========] - loss: 1675.6026776529536\n",
      " Epoch 995/1000; Batch 1/1: [===========] - loss: 1674.7280710196117\n",
      " Epoch 996/1000; Batch 1/1: [===========] - loss: 1673.858684855142\n",
      " Epoch 997/1000; Batch 1/1: [===========] - loss: 1672.9943673539954\n",
      " Epoch 998/1000; Batch 1/1: [===========] - loss: 1672.1373193167785\n",
      " Epoch 999/1000; Batch 1/1: [===========] - loss: 1671.2874394056068\n",
      " Epoch 1000/1000; Batch 1/1: [===========] - loss: 1670.4431223046477\n"
     ]
    }
   ],
   "source": [
    "nn = NeauralNetwork(layers=[\n",
    "        Layer(units=10, input_layer=True),\n",
    "        # Layer(units=40, activation=\"sigmoid\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=1),\n",
    "    ],\n",
    "    loss_function = \"mse\",\n",
    "    learning_rate=0.0001, \n",
    "    verbose=True,\n",
    "    optimizer=\"adam\",\n",
    "    batch_size = 32,\n",
    "    epochs=1000\n",
    ")\n",
    "\n",
    "y_diab = y_diab.reshape(-1, 1) # Network requirement\n",
    "\n",
    "nn.fit(X_diab, y_diab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
