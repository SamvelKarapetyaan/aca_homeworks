{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \"\"\"\n",
    "    Represents a dense layer in a neural network.\n",
    "\n",
    "    Args:\n",
    "        size (int): Number of neurons in the layer.\n",
    "        input_layer (bool, optional): Whether the layer is an input layer. Defaults to False.\n",
    "        activation (str, optional): Activation function for the layer. Defaults to \"linear\".\n",
    "        use_bias (bool, optional): Whether to use biases in the layer. Defaults to True.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            size, \n",
    "            *, \n",
    "            input_layer: bool = False,\n",
    "            activation: str = \"linear\",\n",
    "            use_bias: bool = True,\n",
    "            ):\n",
    "        self.size = size\n",
    "        self.input_layer = input_layer\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self._input = None  # Placeholder for input data\n",
    "        self._output = None  # Placeholder for output data\n",
    "\n",
    "        self.w = None  # Weights matrix\n",
    "        self._weight_gradient = None  # Gradient of weights matrix\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = None  # Our favorite bias vector\n",
    "            self._bias_gradient = None  # Gradient of biases\n",
    "\n",
    "    def _weightInit(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of the layer.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of neurons in the previous layer.\n",
    "        \"\"\"\n",
    "        if self.input_layer:\n",
    "            return  # Input layer doesn't require weights\n",
    "\n",
    "        # Initialize weights matrix using a normal distribution with mean 0 and variance 1 / input_size\n",
    "        self.w = np.random.normal(loc=0, scale=1 / input_size, size=(input_size, self.size))\n",
    "\n",
    "        # Initialize biases as zeros\n",
    "        self.bias = np.zeros((1, self.size))\n",
    "\n",
    "    def activationFunction(self, z):\n",
    "        \"\"\"\n",
    "        Applies the activation function to the input.\n",
    "\n",
    "        Args:\n",
    "            z (ndarray): Input values.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Output values after applying the activation function.\n",
    "        \"\"\"\n",
    "        if self.activation == \"linear\":\n",
    "            return z\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(z, np.zeros(z.shape))\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "    def _activationDerivative(self):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Derivative of the activation function for the layer's output.\n",
    "        \"\"\"\n",
    "        if self.activation == \"linear\":\n",
    "            return 1\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return (self._output > 0) * 1\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self._output * (1 - self._output)\n",
    "\n",
    "    def _setGrad(self, grad):\n",
    "        \"\"\"\n",
    "        Sets the gradients of weights and biases based on the given gradient.\n",
    "\n",
    "        Args:\n",
    "            grad (ndarray): Gradient of the layer's output with respect to the subsequent layer.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Gradient of the layer's output with respect to the current layer's input.\n",
    "        \"\"\"\n",
    "        grad = grad * self._activationDerivative()\n",
    "        self._weight_gradient = self._input.T @ grad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self._bias_gradient = grad.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return grad @ self.w.T\n",
    "    \n",
    "    def _updateGrad(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.w -= learning_rate * self._weight_gradient\n",
    "        if self.use_bias:\n",
    "            self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Computes the output of the layer given an input.\n",
    "\n",
    "        Args:\n",
    "            X (ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Output of the layer.\n",
    "        \"\"\"\n",
    "        if self.input_layer:\n",
    "            return X\n",
    "        \n",
    "        self._input = X\n",
    "        self._output = self.activationFunction(X @ self.w + self.bias)\n",
    "\n",
    "        return self._output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    \"\"\"\n",
    "    Represents a 2D convolutional layer in a neural network.\n",
    "\n",
    "    Args:\n",
    "        size (int): Number of output channels (number of filters).\n",
    "        kernel_size (tuple): Size of the convolutional kernel (height, width).\n",
    "        stride (int, optional): Stride for the convolution operation. Defaults to 1.\n",
    "        activation (str, optional): Activation function for the layer. Defaults to \"linear\".\n",
    "        global_pooling (str, optional): Global pooling operation to apply. Defaults to None.\n",
    "        use_bias (bool, optional): Whether to use biases in the layer. Defaults to True.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            size: int,\n",
    "            kernel_size: tuple,\n",
    "            *,\n",
    "            stride: int = 1,\n",
    "            activation: str = \"linear\",\n",
    "            global_pooling: str = None,\n",
    "            use_bias: bool = True,\n",
    "        ):\n",
    "        self.size = size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.activation = activation\n",
    "        self.global_pooling = global_pooling\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel = None\n",
    "\n",
    "        self._kernel_gradient = None # Gradient of the kernel\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = None # Our favorite bias vector\n",
    "            self._bias_gradient = None # Gradient of biases\n",
    "\n",
    "    def _weightInit(self, depth):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of the layer.\n",
    "\n",
    "        Args:\n",
    "            depth (int): Depth (number of channels) of the input tensor.\n",
    "        \"\"\"\n",
    "        self.kernel_size = self.kernel_size + (depth, self.size)\n",
    "\n",
    "        # Initialize kernel using a normal distribution\n",
    "        self.kernel = np.random.random((self.kernel_size))        \n",
    "\n",
    "        self._kernel_gradient = np.zeros_like(self.kernel)\n",
    "\n",
    "        if self.use_bias:\n",
    "            # Initialize biases as zeros\n",
    "            self.bias = np.zeros((self.size, 1))\n",
    "\n",
    "            self._bias_gradient = np.zeros_like(self.bias)\n",
    "\n",
    "    def activationFunction(self, z):\n",
    "        \"\"\"\n",
    "        Applies the activation function to the input.\n",
    "\n",
    "        Args:\n",
    "            z (ndarray): Input values.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Output values after applying the activation function.\n",
    "        \"\"\"\n",
    "        if self.activation == \"linear\":\n",
    "            return z\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(z, 0)\n",
    "        \n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _activationDerivative(self):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Derivative of the activation function for the layer's output.\n",
    "        \"\"\"\n",
    "        if self.activation == \"linear\":\n",
    "            return 1\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return (self._output > 0) * 1\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self._output * (1 - self._output)\n",
    "\n",
    "    def _setGrad(self, grad):\n",
    "        \"\"\"\n",
    "        Sets the gradients of weights and biases based on the given gradient.\n",
    "\n",
    "        Args:\n",
    "            grad (ndarray): Gradient of the layer's output with respect to the subsequent layer.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Gradient of the layer's output with respect to the current layer's input.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.global_pooling == \"average\":\n",
    "            grad = grad.T[np.newaxis, np.newaxis, ...] # Do some magic\n",
    "            output_gradient = np.ones(self.output_shape) / (self.output_shape[0] * self.output_shape[1])\n",
    "        else:\n",
    "            output_gradient = np.ones(self.output_shape)\n",
    "\n",
    "        if self.use_bias:\n",
    "            # Compute gradient of biases\n",
    "            self._bias_gradient = (grad * output_gradient).reshape(grad.shape[-2], -1).sum(axis=1, keepdims=True) # Do some magic\n",
    "        \n",
    "        output_gradient = output_gradient * self._activationDerivative() * grad\n",
    "\n",
    "        self._kernel_gradient = np.zeros_like(self.kernel)\n",
    "        self._input_gradient = np.zeros_like(self._input)\n",
    "        \n",
    "        for index in range(self._input.shape[-1]):\n",
    "            for i in range(len(self._indices_axis1)):\n",
    "                    x_1, x_2 = self._indices_axis1[i]\n",
    "                    for j in range(len(self._indices_axis2)):\n",
    "                        y_1, y_2 = self._indices_axis2[j]\n",
    "\n",
    "                        # Update kernel gradient using chain rule (May be something wrong here)\n",
    "                        self._kernel_gradient += self._input[x_1:x_2, y_1:y_2, :, [index]] * output_gradient[i, j, :, [index]]\n",
    "                        self._input_gradient[x_1:x_2, y_1:y_2, :, [index]] += (self.kernel * output_gradient[i, j, :, [index]]).sum(axis=3, keepdims=True)\n",
    "\n",
    "        return self._input_gradient\n",
    "\n",
    "    def _updateGrad(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and biases based on their gradients.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for the update.\n",
    "        \"\"\"\n",
    "\n",
    "        self.kernel -= learning_rate * self._kernel_gradient\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the convolutional layer.\n",
    "\n",
    "        Args:\n",
    "            tensor (ndarray): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Output of the convolutional layer.\n",
    "        \"\"\"\n",
    "        self._input = tensor\n",
    "\n",
    "        input_shape = np.array(tensor.shape)\n",
    "\n",
    "        feature_map_shape = ((input_shape[:2]  - self.kernel_size[:2]) / self.stride).astype(int) + 1\n",
    "        self.output_shape = np.concatenate([feature_map_shape, [self.size, input_shape[-1]]]) # -> (feature_map_shape, filters, images)\n",
    "\n",
    "        self._output = np.zeros(self.output_shape)\n",
    "\n",
    "        self._indices_axis1 = [(i - self.kernel_size[0], i) for i in range(self.kernel_size[0], input_shape[0] + 1, self.stride)]\n",
    "        self._indices_axis2 = [(i - self.kernel_size[1], i) for i in range(self.kernel_size[1], input_shape[1] + 1, self.stride)]\n",
    "\n",
    "        for index in range(input_shape[-1]):\n",
    "            for i in range(len(self._indices_axis1)):\n",
    "                    x_1, x_2 = self._indices_axis1[i]\n",
    "                    for j in range(len(self._indices_axis2)):\n",
    "                        y_1, y_2 = self._indices_axis2[j]\n",
    "\n",
    "                        self._output[i, j, :, index] = (self._input[x_1:x_2, y_1:y_2, :, [index]] * self.kernel).sum(axis=(0, 1, 2))\n",
    "        \n",
    "        self._output = self.activationFunction(self._output)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self._output += self.bias\n",
    "\n",
    "        if self.global_pooling == \"average\":\n",
    "            return self._output.mean(axis=(0, 1)).T\n",
    "\n",
    "        return self._output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A class representing a Neural Network.\n",
    "\n",
    "    Attributes:\n",
    "        layers (list): List of Layer objects representing the network layers.\n",
    "        loss_function (str): Loss function to be used for training (default: \"mse\").\n",
    "        learning_rate (float): Learning rate for gradient descent optimization (default: 0.01).\n",
    "        verbose (bool): Flag indicating whether to print progress during training (default: True).\n",
    "        input_depth (int): Number of channels in the input data (default: 3).\n",
    "        epochs (int): Number of training epochs (default: 1).\n",
    "        batch_size (int): Size of the training batches (default: 32).\n",
    "\n",
    "    Methods:\n",
    "        lossFunction(y_true, y_pred):\n",
    "            Computes the loss function value for the given true and predicted labels.\n",
    "\n",
    "        fit(X, y):\n",
    "            Trains the neural network on the provided input and output data.\n",
    "\n",
    "        predict(X):\n",
    "            Performs forward pass and returns the predicted labels for the input data.\n",
    "\n",
    "        forward(X):\n",
    "            Performs forward propagation through the network layers and returns the output.\n",
    "\n",
    "        backward(y_pred, y_true):\n",
    "            Performs backward propagation to update the gradients and weights of the network layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            layers: list,\n",
    "            loss_function: str = \"mse\",\n",
    "            learning_rate=0.01,\n",
    "            verbose: bool = True,\n",
    "            input_depth: int = 3,\n",
    "            epochs: int = 1,\n",
    "            batch_size: int = 32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a NeuralNetwork instance with the provided parameters.\n",
    "\n",
    "        Args:\n",
    "            layers (list): List of Layer objects representing the network layers.\n",
    "            loss_function (str): Loss function to be used for training (default: \"mse\").\n",
    "            learning_rate (float): Learning rate for gradient descent optimization (default: 0.01).\n",
    "            verbose (bool): Flag indicating whether to print progress during training (default: False).\n",
    "            input_depth (int): Number of channels in the input data (default: 3).\n",
    "            epochs (int): Number of training epochs (default: 1).\n",
    "            batch_size (int): Size of the training batches (default: 32).\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.input_depth = input_depth\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Weights initializing:\n",
    "        self.layers[0]._weightInit(self.input_depth)\n",
    "\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i]._weightInit(self.layers[i - 1].size)\n",
    "            # Initialize weights for each layer\n",
    "\n",
    "    def lossFunction(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the loss function value for the given true and predicted labels.\n",
    "\n",
    "        Args:\n",
    "            y_true: True labels.\n",
    "            y_pred: Predicted labels.\n",
    "\n",
    "        Returns:\n",
    "            The computed loss function value.\n",
    "        \"\"\"\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 0.5 * np.mean(np.linalg.norm(y_pred - y_true, axis=1) ** 2)\n",
    "\n",
    "        if self.loss_function == \"cross_entropy\":\n",
    "            self.probabilities_ = np.exp(y_pred - y_pred.max(axis=1, keepdims=True)) # avoid overflow\n",
    "            self.probabilities_ = self.probabilities_ / self.probabilities_.sum(axis=1, keepdims=True)\n",
    "\n",
    "            return -(np.log(self.probabilities_[np.arange(y_true.shape[0]), np.argmax(y_true, axis=1)])).mean()\n",
    "\n",
    "        # Add other loss functions here\n",
    "\n",
    "    def _lossFunctionDerivative(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the loss function with respect to the predicted labels.\n",
    "\n",
    "        Args:\n",
    "            y_pred: Predicted labels.\n",
    "            y_true: True labels.\n",
    "\n",
    "        Returns:\n",
    "            The computed derivative of the loss function.\n",
    "        \"\"\"\n",
    "        if self.loss_function == \"mse\":\n",
    "            derivative = 1 / len(y_pred) * (y_pred - y_true)\n",
    "\n",
    "        if self.loss_function == \"cross_entropy\":\n",
    "            return 1 / len(y_true) * (self.probabilities_ - y_true)\n",
    "\n",
    "        return derivative\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the neural network on the provided input and output data.\n",
    "\n",
    "        Args:\n",
    "            X: Input data.\n",
    "            y: Output data.\n",
    "        \"\"\"\n",
    "        batch_separation = [(i, i + self.batch_size) for i in range(0, X.shape[-1], self.batch_size)]  # Get batch indices\n",
    "        epoch_len = len(batch_separation)\n",
    "\n",
    "        indices = np.arange(X.shape[-1])\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            np.random.shuffle(indices)  # Shuffle the training data\n",
    "\n",
    "            for iter, (i, j) in enumerate(batch_separation):\n",
    "                X_ = X[:, :, :, indices[i:j]]  # Get current batch\n",
    "                y_ = y[indices[i:j]]  # Get current batch\n",
    "\n",
    "                pred = self.forward(X_)\n",
    "\n",
    "                loss = self.lossFunction(y_, pred)\n",
    "\n",
    "                if self.verbose:\n",
    "                    accuracy = (pred.argmax(axis=1) == y_.argmax(axis=1)).mean()\n",
    "                    process_percent = int(iter / epoch_len * 10)\n",
    "                    print(\n",
    "                        f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter}/{epoch_len}: [{process_percent * '=' + '>' + (10 - process_percent) * '-'}] - loss: {loss}; accuracy: {accuracy}\",\n",
    "                        end='',\n",
    "                    )\n",
    "\n",
    "                self.backward(pred, y_)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter + 1}/{epoch_len}: [{11 * '='}] - loss: {loss}; accuracy: {accuracy}\"\n",
    "                )\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward pass and returns the predicted labels for the input data.\n",
    "\n",
    "        Args:\n",
    "            X: Input data.\n",
    "\n",
    "        Returns:\n",
    "            The predicted labels.\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward propagation through the network layers and returns the output.\n",
    "\n",
    "        Args:\n",
    "            X: Input data.\n",
    "\n",
    "        Returns:\n",
    "            The output of the network.\n",
    "        \"\"\"\n",
    "        X_ = np.copy(X)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            X_ = layer(X_)\n",
    "\n",
    "        return X_\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Performs backward propagation to update the gradients and weights of the network layers.\n",
    "\n",
    "        Args:\n",
    "            y_pred: Predicted labels.\n",
    "            y_true: True labels.\n",
    "        \"\"\"\n",
    "        gradient = self._lossFunctionDerivative(y_pred, y_true)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer._setGrad(gradient)\n",
    "            layer._updateGrad(self.learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Data and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 227, 3, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "labels = {\n",
    "    \"cucumber\": 0,\n",
    "    \"eggplant\": 1,\n",
    "    \"mushroom\": 2,\n",
    "}\n",
    "\n",
    "path = \"./test_data/train/\"\n",
    "\n",
    "X = np.concatenate([np.asarray(Image.open(path + image_path))[..., np.newaxis] / 255 for image_path in os.listdir(path)], axis=3)\n",
    "\n",
    "y_cat = np.array([label.split('_')[0] for label in os.listdir(path)]).reshape(-1, 1)\n",
    "y = ohe.fit_transform(y_cat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something wrong with this implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1307.44488087 1293.79982066 1307.66598922]\n",
      " [ 173.26797985  171.84095641  173.16516052]\n",
      " [1172.19346603 1159.14365863 1172.70036979]\n",
      " [1089.85241381 1078.22640817 1090.06239851]\n",
      " [1293.82891619 1281.05812863 1294.77428272]\n",
      " [1365.16713784 1351.07435231 1365.42537565]\n",
      " [ 913.02097903  904.10217958  913.40962289]\n",
      " [ 977.8291397   966.3322791   977.69160044]\n",
      " [ 871.65412004  862.5968242   871.92537748]\n",
      " [1041.95733012 1032.42178396 1042.82584154]]\n",
      " Epoch 1/5; Batch 1/1: [===========] - loss: 3.798649274570498; accuracy: 0.1\n",
      "[[1090.2892465  1078.64788354 1089.20409045]\n",
      " [1042.37404883 1032.82513018 1042.0057766 ]\n",
      " [1307.96854284 1294.3054488  1306.63669911]\n",
      " [ 913.38586982  904.45525146  912.69166023]\n",
      " [ 978.22254069  966.71045613  976.92002241]\n",
      " [1172.66381252 1159.59671454 1171.77696739]\n",
      " [ 173.33715301  171.90814228  173.02880149]\n",
      " [1365.71373333 1351.60230874 1364.35082374]\n",
      " [1294.34663737 1281.55848648 1293.7562037 ]\n",
      " [ 872.00263618  862.93384631  871.23983923]]\n",
      " Epoch 2/5; Batch 1/1: [===========] - loss: 3.275102700811078; accuracy: 0.6\n",
      "[[1172.65429403 1160.04951314 1171.33368728]\n",
      " [ 173.33549301  171.97528953  172.96331423]\n",
      " [ 913.37772758  904.8081221   912.34693183]\n",
      " [1365.70197885 1352.12996444 1363.83492251]\n",
      " [1042.36470833 1033.22824607 1041.61200122]\n",
      " [ 871.99495077  863.27067643  870.91069452]\n",
      " [1294.33538072 1282.05855931 1293.26738752]\n",
      " [1090.28007463 1079.06911897 1088.79202689]\n",
      " [1307.95736607 1294.81078898 1306.1425357 ]\n",
      " [ 978.21494049  967.08841834  976.54966041]]\n",
      " Epoch 3/5; Batch 1/1: [===========] - loss: 3.0642482861723805; accuracy: 0.6\n",
      "[[1307.81829216 1295.315797   1305.7766016 ]\n",
      " [1294.19761769 1282.55830336 1292.90540649]\n",
      " [1365.5567035  1352.65727325 1363.45288905]\n",
      " [ 871.90208343  863.60728509  870.66695321]\n",
      " [ 913.28037397  905.1607606   912.09164695]\n",
      " [ 173.31689144  172.04239225  172.91481308]\n",
      " [ 978.1114621   967.46613274  976.2754244 ]\n",
      " [1090.16425031 1079.49007764 1088.48689254]\n",
      " [1042.25347022 1033.63109626 1041.32038914]\n",
      " [1172.53003015 1160.5020149  1171.00544939]]\n",
      " Epoch 4/5; Batch 1/1: [===========] - loss: 2.8827888664941863; accuracy: 0.6\n",
      "[[1365.35877704 1353.18418499 1363.12390377]\n",
      " [ 913.14784112  905.51313339  911.87180699]\n",
      " [1042.10204986 1034.03364238 1041.06926338]\n",
      " [1307.62878486 1295.8204248  1305.46148109]\n",
      " [1294.00996866 1283.0576711  1292.59368779]\n",
      " [ 977.97017763  967.84356345  976.03927816]\n",
      " [ 871.77562617  863.94364024  870.45705531]\n",
      " [1172.36051946 1160.95417686 1170.72279812]\n",
      " [1090.00637014 1079.9107195  1088.22413085]\n",
      " [ 173.29160911  172.10944401  172.87304365]]\n",
      " Epoch 5/5; Batch 1/1: [===========] - loss: 2.707725185484465; accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers=[\n",
    "        # Conv2d(64, (15, 15), stride=10, activation='relu'),\n",
    "        Conv2d(3, (32, 32), stride=4, activation='relu', global_pooling=\"average\"),\n",
    "        # DenseLayer(3, activation='relu'),\n",
    "    ],\n",
    "    loss_function=\"cross_entropy\",\n",
    "    learning_rate=0.001,\n",
    "    verbose=True,\n",
    "    batch_size=10,\n",
    "    epochs=5,\n",
    ")\n",
    "\n",
    "nn.fit(X[:, :, :, 45:55], y[45:55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(X[:, :, :, 45:55], y[45:55])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
